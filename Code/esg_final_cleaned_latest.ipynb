{"cells":[{"cell_type":"markdown","source":["## Import Required Packages:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b123039e-ef0b-47a3-a385-36ae3147d067"}}},{"cell_type":"code","source":["import os\nimport datetime\nfrom collections import Counter\nfrom pprint import pprint\nimport pandas as pd\nimport pyspark.sql.functions as F\nfrom pyspark.sql import DataFrame as spark_DataFrame\nfrom pyspark.sql.types import *\n# from functools import reduce\nimport matplotlib.pyplot as plt\nimport requests\nimport re\nfrom itertools import product\nimport time\nfrom io import BytesIO\nimport requests\nimport numpy as np\nfrom pyspark.sql import Window\nimport sys\n#---------------------------- additional level2 packages ---------------------------------\nfrom pyspark.sql.column import Column # _to_seq\nfrom pyspark import copy_func, since\nfrom pyspark.context import SparkContext\nimport genericpath\nfrom genericpath import *\nimport stat\nfrom pyspark.sql.column import _to_seq\nimport builtins\n#------------------------------------- base import depencency ---------------------------\nfrom __future__ import annotations\nfrom pandas._libs import (NaT,Period,Timestamp,index as libindex,lib,)\nfrom pandas.util._decorators import (\n    Appender,\n    Substitution,\n    deprecate_kwarg,\n    deprecate_nonkeyword_arguments,\n    doc,\n    rewrite_axis_style_signature,\n)\nfrom pandas._libs.lib import no_default"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d97036f9-4f1d-41a8-a5b2-373977907936"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Utility Functions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5d411e61-d25c-47f2-8908-7e3e7548c725"}}},{"cell_type":"code","source":["# base level data point to start \n_iord_zero_point = object()\n# core engine algorithm for calculation and data arrangement \ndef iord_algo(function, sequence, initial=_iord_zero_point):          \n    \"\"\"\n    iord_algo(function, sequence[, initial]) -> value\n\n    This is function of two arguments cumulatively of items of sequence, in right hand direction,\n    so as to decrease sequence to single value. For example, iord_algo(lambda x, y: x+y, [1, 2, 3, 4, 5]) \n    process ((((1+2)+3)+4)+5).  Incase initial is avilable, it will be placed before the item\n    of the sequence in the process, and serves as a default when for empty sequence.\n    \"\"\"\n\n    iord_point = iter(sequence)\n\n    if initial is _iord_zero_point:\n        try:\n            evaluate = next(iord_point)\n        except StopIteration:\n            raise TypeError(\"iord_algo() of empty sequence with no initial value\") from None\n    else:\n        evaluate = initial\n        \n    for element in iord_point:\n        evaluate = function(evaluate, element)\n\n    return evaluate"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"40ab4205-e2ba-4b7e-afab-37e6a7431a53"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["#to give a lable to column\ndef _apply_grid_label_name(name):\n    sc = SparkContext._active_spark_context\n    return sc._jvm.functions.col(name)\n\nif sys.version >= '3':\n    basestring = str\n    long = int\n\n#to apply label with java jvm call\ndef _jvm_grid_label(col):\n    if isinstance(col, Column):\n        jcol = col._jc\n    elif isinstance(col, basestring):\n        jcol = _apply_grid_label_name(col)\n    else:\n        raise TypeError(\n            \"Invalid argument, should be either string or column: \"\n            \"{0} of type {1}. \"\n            \"For cols literals, please use lit, array, struct or create_map \"\n            \"functions.\".format(col, type(col)))\n    return jcol\n\n# convert date format into proper alignment\ndef date_alignment(date, format):\n    sc = SparkContext._active_spark_context\n    return Column(sc._jvm.functions.date_format(_jvm_grid_label(date), format))\n\n# handle and validate begning date to pass jvm \ndef beginning_date(col, format=None):\n    sc = SparkContext._active_spark_context\n    if format is None:\n        jc = sc._jvm.functions.to_date(_jvm_grid_label(col))\n    else:\n        jc = sc._jvm.functions.to_date(_jvm_grid_label(col), format)\n    return Column(jc)\n\n# to re arrange columns for require format \ndef intelora_rearrange_handler(col):\n    sc = SparkContext._active_spark_context\n    jc = sc._jvm.functions.explode(_jvm_grid_label(col))\n    return Column(jc)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ffc7bc18-090d-448d-a157-8151309cfa88"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# align data with column and grid scale\ndef intelora_data_aligner(                             \n        self,\n        column: IndexLabel,\n        ignore_index: bool = False,\n    ) -> DataFrame:\n        if not self.columns.is_unique:\n            raise ValueError(\"columns is not unique, it has to be be unique-001\")\n\n        columns: list[Hashable]\n        if is_scalar(column) or isinstance(column, tuple):\n            columns = [column]\n        elif isinstance(column, list) and all(\n            map(lambda c: is_scalar(c) or isinstance(c, tuple), column)\n        ):\n            if not column:\n                raise ValueError(\"column is empty, it should be nonempty\")\n            if len(column) > len(set(column)):\n                raise ValueError(\"columns is not unique, it has to be be unique-002\")\n            columns = column\n        else:\n            raise ValueError(\"column is nut amoung any of scalar, tuple, or list thereof \")\n\n        df = self.reset_index(drop=True)\n        if len(columns) == 1:\n            result = df[columns[0]].explode()\n        else:\n            mylen = lambda x: len(x) if is_list_like(x) else -1\n            counts0 = self[columns[0]].apply(mylen)\n            for c in columns[1:]:\n                if not all(counts0 == self[c].apply(mylen)):\n                    raise ValueError(\"columns should match element counts members\")\n            result = DataFrame({c: df[c].explode() for c in columns})\n        result = df.drop(columns, axis=1).join(result)\n        if ignore_index:\n            result.index = default_index(len(result))\n        else:\n            result.index = self.index.take(result.index)\n        result = result.reindex(columns=self.columns, copy=False)\n        return result\n\n# ------------------------ data structure for matching purpose -----------------------------------\ndef intelora_dataset_matcher(                                      \n    obj: DataFrame,\n    func: AggFuncType,\n    axis: Axis = 0,\n    raw: bool = False,\n    result_type: str | None = None,\n    args=None,\n    kwargs=None,\n) -> FrameApply:\n    \"\"\"It build and return a row or column based frame required object\"\"\"\n    axis = obj._get_axis_number(axis)\n    klass: type[FrameApply]\n    if axis == 0:\n        klass = FrameRowApply\n    elif axis == 1:\n        klass = FrameColumnApply\n\n    return klass(\n        obj,\n        func,\n        raw=raw,\n        result_type=result_type,\n        args=args,\n        kwargs=kwargs,\n    )\n# ------------------------------------- clubbing data operation --------------------------------------------\ndef intelora_add(                                     \n        self,\n        func: AggFuncType,\n        axis: Axis = 0,\n        raw: bool = False,\n        result_type=None,\n        args=(),\n        **kwargs,\n    ):\n        # the core appy of data frame matcher\n        op = intelora_dataset_matcher(\n            self,\n            func=func,\n            axis=axis,\n            raw=raw,\n            result_type=result_type,\n            args=args,\n            kwargs=kwargs,\n        )\n        return op.apply().__finalize__(self, method=\"apply\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f9423e24-f7cf-4a93-b777-f86236a7041c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# filter out data and argument with none and index label\n@deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\ndef to_clean_require(\n    self,\n    axis: Axis = 0,\n    how: str = \"any\",\n    thresh=None,\n    subset: IndexLabel = None,\n    inplace: bool = False,\n    ):\n        inplace = validate_bool_kwarg(inplace, \"inplace\")\n        if isinstance(axis, (tuple, list)):\n            raise TypeError(\" it not support anymore supplying multiple axes to axis.\")\n        axis = self._get_axis_number(axis)\n        agg_axis = 1 - axis\n\n        agg_obj = self\n        if subset is not None:\n            # subset require list format only\n            if not is_list_like(subset):\n                subset = [subset]\n            ax = self._get_axis(agg_axis)\n            indices = ax.get_indexer_for(subset)\n            check = indices == -1\n            if check.any():\n                raise KeyError(np.array(subset)[check].tolist())\n            agg_obj = self.take(indices, axis=agg_axis)\n\n        if thresh is not None:\n            count = agg_obj.count(axis=agg_axis)\n            mask = count >= thresh\n        elif how == \"any\":\n            # faster compare to 'agg_obj.count(agg_axis) == self.shape[agg_axis]'\n            mask = notna(agg_obj).all(axis=agg_axis, bool_only=False)\n        elif how == \"all\":\n            # faster compare to 'agg_obj.count(agg_axis) > 0'\n            mask = notna(agg_obj).any(axis=agg_axis, bool_only=False)\n        else:\n            if how is not None:\n                raise ValueError(f\"not valid how option: {how}\")\n            else:\n                raise TypeError(\"should specify how or thresh\")\n\n        if np.all(mask):\n            result = self.copy()\n        else:\n            result = self.loc(axis=axis)[mask]\n\n        if inplace:\n            self._update_inplace(result)\n        else:\n            return result\n\n#------------------------------------- data structure collpse ------------------------------------------------------\ndef intelora_arranger(           \n    self,\n    by=None,\n    axis: Axis = 0,\n    level: Level | None = None,\n    as_index: bool = True,\n    sort: bool = True,\n    group_keys: bool = True,\n    squeeze: bool | lib.NoDefault = no_default,\n    observed: bool = False,\n    dropna: bool = True,\n) -> DataFrameGroupBy:\n    from pandas.core.groupby.generic import DataFrameGroupBy\n    # validating data in sequence\n    if squeeze is not no_default:\n        warnings.warn(\n            (\n               \" squeeze parameter is no longer supported and \"\n                \"suppose to removed later.\"\n            ),\n            FutureWarning,\n            stacklevel=find_stack_level(),\n        )\n    else:\n        squeeze = False\n\n    if level is None and by is None:\n        raise TypeError(\"You have to supply one of 'by' and 'level'\")\n    axis = self._get_axis_number(axis)\n    return DataFrameGroupBy(\n        obj=self,\n        keys=by,\n        axis=axis,\n        level=level,\n        as_index=as_index,\n        sort=sort,\n        group_keys=group_keys,\n        squeeze=squeeze,  # type: ignore[arg-type]\n        observed=observed,\n        dropna=dropna,\n    )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4e32434e-e0c2-46ba-8e69-66d963dac161"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# apply call leavel data predefined rule \ndef also_call_as(c):\n    \"\"\"\n    create list of aliases for given company using predefined rules.\n    Try to encompass as many options as possible keeping so that user can customize,\n    containg aliases may left out.\n    \"\"\"\n    c = c.lower()\n\n    # maintain URLs string such as .com\n    if len(c) > 3 and c[-4] == \".\":\n        a = c\n        c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n        aliases = set([c, a])\n    else:\n        aliases = set([c])\n\n    # get last character of endings\n    if len(c.split()[-1]) == 1:\n        c = c.rsplit(\" \", 1)[0]\n        aliases.add(c)\n\n    # Company legal endings\n    endings = [\"inc\",\n               \"corp\",\n               \"plc\", \n               \"reit\", \n               \"co\", \n               \"cor\", \n               \"group\", \n               \"company\",\n               \"trust\",\n               \"energy\",\n               \"international\",\n               \"of america\",\n               \"pharmaceuticals\",\n               \"clas\",\n               \"in\", \"nv\",\n               \"sa\", \n               \"re\", \n               \"pvt ltd\",\n               \"private limited\" ,\n               \"india private limited\"\n               \"Co\",\n               \"CO.\",\n               \"Companies\",\n               \"Company\",\n               \"Corp\",\n               \"CORP.\",\n               \"Corporation\",\n               \"Inc\",\n               \"INC.\",\n               \"Incorporated\",\n               \"Limited\",\n               \"Ltd\",\n               \"Professional Corporation\",\n               \"Chartered\",\n               \"Limited\",\n               \"Ltd\",\n               \"Ltd.\",\n               \"pa\",\n               \"p.c \",\n               \"Professional Association\",\n               \"Professional Corporation\",\n               \"Professional Service Corporation\",\n               \"psc\",\n               \"sc.\",\n               \"Service Corporation\"]\n    \n    n_endings = 3  # Can have up to 3 of these endings\n    for _ in range(n_endings):\n        aliases.update([a.rsplit(\" \", 1)[0] for a in aliases if\n                        any([a.endswith(\" \" + e) for e in endings])])\n        c = c.rsplit(\" \", 1)[0] if any([c.endswith(\" \" + e) for e in endings]) else c\n\n    # Alias any dashes and replace in company name\n    aliases.update([a.replace(\"-\", \"\") for a in aliases] +\n                   [a.replace(\"-\", \" \") for a in aliases])\n    c = c.replace(\"-\", \" \")\n\n    # If '&' stands on its own, add alias of 'and'\n    aliases.update([a.replace(\" & \", \" and \") for a in aliases])\n    return {c: list(aliases)}\n\n# get column alias from compnay \ndef generate_common_org_names(companies):\n    \"\"\"\n    store data of  companies and loop through them to find aliases\n    \"\"\"\n    #companies = [org_name]\n    comp_dict = dict()\n    for c in companies:\n        comp_dict.update(also_call_as(c))\n    return comp_dict\n\n#using spark dataframe union for list\ncombine_spark_dfs = lambda sdf_list: iord_algo(spark_DataFrame.union, sdf_list)\n\n# a global level common data defination\nclass Common_MetaData:\n    \"\"\" Variables to use across many functions. \"\"\"\n    keep = [\"DATE\",\n            \"SourceCommonName\", \n            \"DocumentIdentifier\", \n            \"Themes\",\n            \"Organizations\",\n            \"V2Tone\"]\n    tone = [\"Tone\",\n            \"PositiveTone\",\n            \"NegativeTone\", \n            \"Polarity\",\n            \"ActivityDensity\", \n            \"SelfDensity\",\n            \"WordCount\"]\n    organizations = None\n\n# spark based expand basic dataset\n@udf(ArrayType(StringType(), True))\ndef simple_expand_spark(x):\n    \"\"\" Expand semicolon token strint to a list (also avoid empty)\"\"\"\n    if not x:\n        return []\n    return list(filter(None, x.split(\";\")))\n\n# spark based tone data generation\n@udf(MapType(StringType(), DoubleType()))\ndef tone_expand_spark(x):\n    \"\"\" Expand and identify tone field. \"\"\"\n    if not x:\n        return {t: None for t in Common_MetaData.tone}\n    return {Common_MetaData.tone[i]: float(v) for i, v in enumerate(x.split(\",\"))}\n\n# spark template validation for stream\n@udf(BooleanType())\ndef has_theme_spark(x, theme):\n    \"\"\" Is the given theme included in any of the listed themes? \"\"\"\n    return any([theme in lst.split(\"_\") for lst in x])\n\n# cleanup process for org data \n@udf(StringType())\ndef clean_organization(s):\n    \"\"\" Standardize the organization names. \"\"\"\n    for k, v in Common_MetaData.organizations.items():\n        if v[0] in s.split() :\n            return k\n    return s.lower()\n\n## alias of company and compass data value\n# def also_call_create(c):\n#     \"\"\"\n#     create list of aliases for require company using few predefined rules.\n#     Try to encompass many options with possible keeping possibality,\n#     keep needed aliases left out.\n#     \"\"\"\n#     c = c.lower()\n\n#     # URLs (e.g. \".com\")\n#     if len(c) > 3 and c[-4] == \".\":\n#         a = c\n#         c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n#         aliases = set([c, a])\n#     else:\n#         aliases = set([c])\n#     return aliases"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aedc92c1-8c20-406f-b993-2ba95ecabd4c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## Downloading , Filtering and Storing Gdelt Data:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e0a731b-1a7d-49a2-b339-26b9b6f32a90"}}},{"cell_type":"code","source":["# Data structure format to help to arrange data in various format\nclass Redesign_data_format:\n    # spark code to redesign data frame based on matadata  \n    def redesign_sdf(self, sdf, file_path_refined, org_name):\n        \"\"\"\n        Provide spark data frame of the downloaded data and then reformat\n        it into human-readable all Common_MetaData.\n        Also it add few more Common_MetaData for structuring purposes.\n        \"\"\"\n        setattr(Common_MetaData, \"organizations\",generate_common_org_names(org_name))\n        \n        sdf = sdf.select(*Common_MetaData.keep)\n        \n        if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n            # manage existing columns\n            sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n                      .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n                      .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n                      .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n                   )\n\n            # Create ESG columns & explode organization column\n            sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n                      .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n                      .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n                   )\n        \n            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n            print(\"sdf created\")\n        \n        else:\n            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n        \n        sdf = (sdf.withColumn(\"Organization\", intelora_rearrange_handler(\"Organizations\"))\n                  .withColumn(\"Organization\", clean_organization(\"organization\"))\n                  .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n              )\n\n        # Expand tone columns\n        exprs = [F.col(\"V2Tone\").getItem(k).alias(k) for k in Common_MetaData.tone]\n\n        sdf = sdf.select(*sdf.columns, *exprs).drop(\"V2Tone\")\n        #print(sdf.column)\n        return sdf\n\n## download gdelt dat and generate table\n#     def download_and_generate_gdelt_table1(self, date, gd):\n#         \"\"\"\n#         Download the GDELT table as a pandas dataframe using the gdelt package.\n#         Return a spark data frame.\n#         \"\"\"\n#         pdf = gd.Search([date], table=\"gkg\",coverage=True, output=\"df\")\n#         pdf[\"DATE\"] = pd.to_datetime(pdf[\"DATE\"], format=\"%Y%m%d%H%M%S\")\n\n#         sdf = spark.createDataFrame(pdf)\n#         print(\"   * loaded *  \", date)\n#         return sdf\n    # download gdelt dat and generate table helper function\n    def download_and_generate_gdelt_table(self, date, file_path):\n        file_path_os =  file_path.replace(\"dbfs:/\", \"/dbfs/\")\n        s = []\n        for i in range(24):\n            for j in list(range(0, 60, 15)):\n                if i<10:\n                    if j<10:\n                        s.append('0' + str(i) + '0' + str(j))\n                    else:\n                        s.append('0' + str(i) + str(j))\n                else:\n                    if j<10:\n                        s.append(str(i) + '0' + str(j))\n                    else:\n                         s.append(str(i) + str(j))\n        \n        #validing file path URL\n        if not os.path.exists(file_path_os):\n            li = ''.join(date.split('-'))\n            c=0\n            df1 = None\n            for elem in s:\n                try:\n                    print(li, elem)\n                    response = requests.get('http://data.gdeltproject.org/gdeltv2/'+li+ elem + '00.gkg.csv.zip')\n                    #for example actual url is = 'http://data.gdeltproject.org/gkg/20220204.gkg.csv.zip')\n                    buffer = BytesIO(response.content)\n                   \n                    frame = pd.read_csv(buffer, compression='zip', sep='\\t',header=None, warn_bad_lines=True,encoding='latin')\n                    frame[1] = pd.to_datetime(frame[1], format=\"%Y%m%d%H%M%S\")\n        \n                    frame.columns = ['GKGRECORDID', \n                                     'DATE', \n                                     'SourceCollectionIdentifier', \n                                     'SourceCommonName',\n                                     'DocumentIdentifier',\n                                     'Counts',\n                                     'V2Counts', \n                                     'Themes', \n                                     'V2Themes',\n                                     'Locations',\n                                     'V2Locations', \n                                     'Persons', \n                                     'V2Persons', \n                                     'Organizations',\n                                     'V2Organizations', \n                                     'V2Tone',\n                                     'Dates',\n                                     'GCAM',\n                                     'SharingImage',\n                                     'RelatedImages', \n                                     'SocialImageEmbeds', \n                                     'SocialVideoEmbeds', \n                                     'Quotations',\n                                     'AllNames', \n                                     'Amounts', \n                                     'TranslationInfo', \n                                     'Extras'] \n                    columns1 = ['DATE', \n                                'SourceCollectionIdentifier', \n                                'SourceCommonName',\n                                'DocumentIdentifier',\n                                'Counts', \n                                'V2Counts',\n                                'Themes',\n                                'V2Themes',\n                                'Locations',\n                                'V2Locations',\n                                'Organizations',\n                                'V2Organizations',\n                                'V2Tone',\n                                'Dates'] \n                    frame = frame[columns1]\n                    print(frame.shape)\n                    if c==0:\n                        df1 = frame\n                        c=1\n                    else:\n                        df1 = df1.append(frame, ignore_index=True)\n                except:\n                    pass\n            \n            sdf = spark.createDataFrame(df1)\n            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path)\n            print(\"   * loaded *  \", date)\n        else:\n            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path)\n        return sdf\n\n    # function to get all data for input organization\n    def getting_all_org_data(self, start_date, end_date, main_dir , refined_dir,organisation_name = None, override_save=False):\n        \"\"\"\n        For each date between start_date and end_date, either download\n        and clean the data or load the pre-saved data. Save the day's data\n        in case of future use (so it doesn't have to be downloaded and cleaned again)\n        \"\"\"\n        print(\"Loading and cleaning all data\")\n        data_list = []\n        \n        # Download and format the daily data\n        for i, date in enumerate(pd.date_range(start_date, end_date).astype(str)):\n            if i % 7 == 1:\n                # Prevent it hanging like it does sometimes\n                time.sleep(60)\n\n            try:\n                #file_path = os.path.join(main_dir, date)\n                file_path = os.path.join(main_dir, date)\n                file_path_refined = os.path.join(refined_dir, date)\n                df = self.redesign_sdf(self.download_and_generate_gdelt_table(date, file_path), file_path_refined, organisation_name)      \n                data_list.append(df)\n                del df\n                spark.catalog.clearCache()\n\n            except Exception as e:\n              print(f\"!!! Failed to complete {date}!\")\n              print(\"  ****   Reason:\\n\" + str(e) + \"\\n\\n\")\n        return combine_spark_dfs(data_list)\n\n## function to download and formate data into human readable format\n#     def reformat_sdf(self, sdf, file_path_refined, org_name):\n#         \"\"\"\n#         Given a spark data frame of the downloaded data, reformat it\n#         into human-readable Common_MetaData.\n#         Add a few more Common_MetaData for our purposes.\n#         \"\"\"\n#         sdf = sdf.select(*Common_MetaData.keep)\n        \n#         if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n#             # Reformat existing columns\n#             sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n#                       .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n#                       .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n#                       .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n#                    )\n\n#             # Create ESG columns & explode organization column\n#             sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n#                       .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n#                       .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n#                    )\n        \n#             sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n#             print(\"sdf created\")\n        \n#         else:\n#             sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n        \n#         sdf = (sdf.withColumn(\"Organization\", F.explode(\"Organizations\"))\n#                   .withColumn(\"Organization\", clean_organization(\"organization\"))\n#                   .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n#               )\n\n#         return sdf\n\n# gdelt data pre process to filter out for process before start\n#      def preprocess_gdelt_data(self ,start_date, end_date, org_name = None, save_csv=True):\n#         \"\"\"\n#         \"\"\"   \n#         dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n\n#         base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n#         if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n#             dbutils.fs.mkdirs(base_dir)\n\n#         org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n#         if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n#             dbutils.fs.mkdirs(org_dir)\n\n#         base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n#         base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n\n#         if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n#             dbutils.fs.mkdirs(base_data_dir)\n#         if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n#             dbutils.fs.mkdirs(base_data_dir_refined)\n#         # Download and reformat the data\n#         print('')\n#         data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n#         print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n#               f\"organizations from {start_date} to {end_date}\")\n\n#         # Save the data\n#         print(\"Saving Data...\")\n#         data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n#         if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n#             dbutils.fs.mkdirs(base_data_dir)\n#         data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n#         print(f\"Saved to {data_save_path}\")\n\n#         return data\n#     # COMMAND ----------\n\n# connector function to connect with gddelt and pull data\ndef download_gdelt_data(start_date, end_date, org_name = None, save_csv=True):\n    \"\"\"\n    \"\"\"   \n    dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n\n    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n    if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n        dbutils.fs.mkdirs(base_dir)\n\n    org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n    if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n        dbutils.fs.mkdirs(org_dir)\n\n    base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n    base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n\n    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n        dbutils.fs.mkdirs(base_data_dir)\n    if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n        dbutils.fs.mkdirs(base_data_dir_refined)\n        \n    # Download and reformat the data\n    print('')\n    data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n    print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n          f\"organizations from {start_date} to {end_date}\")\n\n    # Save the data\n    print(\"Saving Data...\")\n    data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n        dbutils.fs.mkdirs(base_data_dir)\n    data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n    print(f\"Saved-001 to {data_save_path}\")\n\n    return data\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3ed691b2-9044-4bd4-ba48-3d72ef2052d4"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["## ESG Computating Functions:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b143a2ba-b485-4650-a335-14d60bceeb7c"}}},{"cell_type":"code","source":["from __future__ import annotations\nfrom pandas.core.dtypes.generic import (\n    ABCDataFrame,\n    ABCSeries,\n)\n\nGlobal_identifierlabel = \"\"\"\n        Parameters\n        ----------\n        buf : str, Path or StringIO-like, optional, default None\n            Buffer to write to. If None, the output is returned as a string.\n        columns : sequence, optional, default None\n            The subset of columns to write. Writes all columns by default.\n        col_space : %(col_space_type)s, optional\n            %(col_space)s.\n        header : %(header_type)s, optional\n            %(header)s.\n        index : bool, optional, default True\n            Whether to print index (row) labels.\n        na_rep : str, optional, default 'NaN'\n            String representation of ``NaN`` to use.\n        formatters : list, tuple or dict of one-param. functions, optional\n            Formatter functions to apply to columns' elements by position or\n            name.\n            The result of each function must be a unicode string.\n            List/tuple must be of length equal to the number of columns.\n        float_format : one-parameter function, optional, default None\n            Formatter function to apply to columns' elements if they are\n            floats. This function must return a unicode string and will be\n            applied only to the non-``NaN`` elements, with ``NaN`` being\n            handled by ``na_rep``.\n\n            .. versionchanged:: 1.2.0\n\n        sparsify : bool, optional, default True\n            Set to False for a DataFrame with a hierarchical index to print\n            every multiindex key at each row.\n        index_names : bool, optional, default True\n            Prints the names of the indexes.\n        justify : str, default None\n            How to justify the column labels. If None uses the option from\n            the print configuration (controlled by set_option), 'right' out\n            of the box. Valid values are\n\n            * left\n            * right\n            * center\n            * justify\n            * justify-all\n            * start\n            * end\n            * inherit\n            * match-parent\n            * initial\n            * unset.\n        max_rows : int, optional\n            Maximum number of rows to display in the console.\n        max_cols : int, optional\n            Maximum number of columns to display in the console.\n        show_dimensions : bool, default False\n            Display DataFrame dimensions (number of rows by number of columns).\n        decimal : str, default '.'\n            Character recognized as decimal separator, e.g. ',' in Europe.\n    \"\"\"\nGive_identifierlabel = \"\"\"\n        Returns\n        -------\n        str or None\n            If buf is None, returns the result as a string. Otherwise returns\n            None.\n    \"\"\"\nclass InteloraRecordHandler:\n    \"\"\"Class for processing dataframe formatting options and data.\"\"\"\n\n    __doc__ = __doc__ if __doc__ else \"\"\n    __doc__ += Global_identifierlabel + Give_identifierlabel\n\n    def __init__(\n        self,\n        frame: DataFrame,\n        columns: Sequence[str] | None = None,\n        col_space: ColspaceArgType | None = None,\n        header: bool | Sequence[str] = True,\n        index: bool = True,\n        na_rep: str = \"NaN\",\n        formatters: FormattersType | None = None,\n        justify: str | None = None,\n        float_format: FloatFormatType | None = None,\n        sparsify: bool | None = None,\n        index_names: bool = True,\n        max_rows: int | None = None,\n        min_rows: int | None = None,\n        max_cols: int | None = None,\n        show_dimensions: bool | str = False,\n        decimal: str = \".\",\n        bold_rows: bool = False,\n        escape: bool = True,\n    ):\n        self.frame = frame\n        self.columns = self._initl_cols(columns)\n        self.col_space = self._initl_colsp(col_space)\n        self.header = header\n        self.index = index\n        self.na_rep = na_rep\n        self.formatters = self._initl_metadata_type(formatters)\n        self.justify = self._initl_caliber(justify)\n        self.float_format = float_format\n        self.sparsify = self._initl_distribute(sparsify)\n        self.show_index_names = index_names\n        self.decimal = decimal\n        self.bold_rows = bold_rows\n        self.escape = escape\n        self.max_rows = max_rows\n        self.min_rows = min_rows\n        self.max_cols = max_cols\n        self.show_dimensions = show_dimensions\n\n        self.max_cols_fitted = self._get_cols_max_size()\n        self.max_rows_fitted = self._get_rows_max_size()\n\n        self.tr_frame = self.frame\n        self.intelora_triming_data()\n        self.adj = get_adjustment()\n\n    def intelora_char_getcols(self) -> list[list[str]]:\n        \"\"\"\n        Render a DataFrame to a list of columns (as lists of strings).\n        \"\"\"\n        strcols = self._get_index_un_strcols()\n\n        if self.index:\n            str_index = self._get_index_metadata_type(self.tr_frame)\n            strcols.insert(0, str_index)\n\n        return strcols\n\n    @property\n    def matrix_to_display(self) -> bool:\n        return self.show_dimensions is True or (\n            self.show_dimensions == \"truncate\" and self.check_trimed\n        )\n\n    @property\n    def check_trimed(self) -> bool:\n        return bool(self.check_htrimed or self.check_vtrimed)\n\n    @property\n    def check_htrimed(self) -> bool:\n        return bool(self.max_cols_fitted and (len(self.columns) > self.max_cols_fitted))\n\n    @property\n    def check_vtrimed(self) -> bool:\n        return bool(self.max_rows_fitted and (len(self.frame) > self.max_rows_fitted))\n\n    @property\n    def intelora_info_matrix(self) -> str:\n        return f\"\\n\\n[{len(self.frame)} rows x {len(self.frame.columns)} columns]\"\n\n    @property\n    def find_named_index(self) -> bool:\n        return _has_names(self.frame.index)\n\n    @property\n    def find_named_columns(self) -> bool:\n        return _has_names(self.frame.columns)\n\n    @property\n    def display_row_sequence_name(self) -> bool:\n        return all((self.find_named_index, self.index, self.show_index_names))\n\n    @property\n    def display_col_sequence_name(self) -> bool:\n        return all((self.find_named_columns, self.show_index_names, self.header))\n\n    @property\n    def frame_all_rows(self) -> int:\n        return min(self.max_rows or len(self.frame), len(self.frame))\n\n    def _initl_distribute(self, sparsify: bool | None) -> bool:\n        if sparsify is None:\n            return get_option(\"display.multi_sparse\")\n        return sparsify\n\n    def _initl_metadata_type(\n        self, formatters: FormattersType | None\n    ) -> FormattersType:\n        if formatters is None:\n            return {}\n        elif len(self.frame.columns) == len(formatters) or isinstance(formatters, dict):\n            return formatters\n        else:\n            raise ValueError(\n                f\"Formatters length({len(formatters)}) should match \"\n                f\"DataFrame number of columns({len(self.frame.columns)})\"\n            )\n\n    def _initl_caliber(self, justify: str | None) -> str:\n        if justify is None:\n            return get_option(\"display.colheader_justify\")\n        else:\n            return justify\n\n    def _initl_cols(self, columns: Sequence[str] | None) -> Index:\n        if columns is not None:\n            cols = ensure_index(columns)\n            self.frame = self.frame[cols]\n            return cols\n        else:\n            return self.frame.columns\n\n    def _initl_colsp(self, col_space: ColspaceArgType | None) -> ColspaceType:\n        result: ColspaceType\n\n        if col_space is None:\n            result = {}\n        elif isinstance(col_space, (int, str)):\n            result = {\"\": col_space}\n            result.update({column: col_space for column in self.frame.columns})\n        elif isinstance(col_space, Mapping):\n            for column in col_space.keys():\n                if column not in self.frame.columns and column != \"\":\n                    raise ValueError(\n                        f\"Col_space is defined for an unknown column: {column}\"\n                    )\n            result = col_space\n        else:\n            if len(self.frame.columns) != len(col_space):\n                raise ValueError(\n                    f\"Col_space length({len(col_space)}) should match \"\n                    f\"DataFrame number of columns({len(self.frame.columns)})\"\n                )\n            result = dict(zip(self.frame.columns, col_space))\n        return result\n\n    def _get_cols_max_size(self) -> int | None:\n        \"\"\"Number of columns fitting the screen.\"\"\"\n        if not self._intelora_check_command_validator():\n            return self.max_cols\n\n        width, _ = get_terminal_size()\n        if self._check_display_frame(width):\n            return width\n        else:\n            return self.max_cols\n\n    def _get_rows_max_size(self) -> int | None:\n        \"\"\"Number of rows with data fitting the screen.\"\"\"\n        max_rows: int | None\n\n        if self._intelora_check_command_validator():\n            _, height = get_terminal_size()\n            if self.max_rows == 0:\n                # rows available to fill with actual data\n                return height - self._get_row_count_auxillary()\n\n            if self._check_display_small(height):\n                max_rows = height\n            else:\n                max_rows = self.max_rows\n        else:\n            max_rows = self.max_rows\n\n        return self._mix_value_alignment(max_rows)\n\n    def _mix_value_alignment(self, max_rows: int | None) -> int | None:\n        \"\"\"Adjust max_rows using display logic.\n\n        See description here:\n        https://pandas.pydata.org/docs/dev/user_guide/options.html#frequently-used-options\n\n        GH #37359\n        \"\"\"\n        if max_rows:\n            if (len(self.frame) > max_rows) and self.min_rows:\n                # if truncated, set max_rows showed to min_rows\n                max_rows = min(self.min_rows, max_rows)\n        return max_rows\n\n    def _intelora_check_command_validator(self) -> bool:\n        \"\"\"Check if the output is to be shown in terminal.\"\"\"\n        return bool(self.max_cols == 0 or self.max_rows == 0)\n\n    def _check_display_frame(self, max_width) -> bool:\n        return bool(self.max_cols == 0 and len(self.frame.columns) > max_width)\n\n    def _check_display_small(self, max_height) -> bool:\n        return bool(self.max_rows == 0 and len(self.frame) > max_height)\n\n    def _get_row_count_auxillary(self) -> int:\n        \"\"\"Get number of rows occupied by prompt, dots and dimension info.\"\"\"\n        dot_row = 1\n        prompt_row = 1\n        num_rows = dot_row + prompt_row\n\n        if self.show_dimensions:\n            num_rows += len(self.intelora_info_matrix.splitlines())\n\n        if self.header:\n            num_rows += 1\n\n        return num_rows\n\n    def intelora_triming_data(self) -> None:\n        \"\"\"\n        Check whether the frame should be truncated. If so, slice the frame up.\n        \"\"\"\n        if self.check_trimed_horizontally:\n            self._direct_htrim()\n\n        if self.check_trimed_vertically:\n            self._direct_vtrim()\n\n    def _direct_htrim(self) -> None:\n        \"\"\"Remove columns, which are not to be displayed and adjust formatters.\n\n        Attributes affected:\n            - tr_frame\n            - formatters\n            - tr_col_num\n        \"\"\"\n        assert self.max_cols_fitted is not None\n        col_num = self.max_cols_fitted // 2\n        if col_num >= 1:\n            left = self.tr_frame.iloc[:, :col_num]\n            right = self.tr_frame.iloc[:, -col_num:]\n            self.tr_frame = concat((left, right), axis=1)\n\n            # truncate formatter\n            if isinstance(self.formatters, (list, tuple)):\n                self.formatters = [\n                    *self.formatters[:col_num],\n                    *self.formatters[-col_num:],\n                ]\n        else:\n            col_num = cast(int, self.max_cols)\n            self.tr_frame = self.tr_frame.iloc[:, :col_num]\n        self.tr_col_num = col_num\n\n    def _direct_vtrim(self) -> None:\n        \"\"\"Remove rows, which are not to be displayed.\n\n        Attributes affected:\n            - tr_frame\n            - tr_row_num\n        \"\"\"\n        assert self.max_rows_fitted is not None\n        row_num = self.max_rows_fitted // 2\n        if row_num >= 1:\n            head = self.tr_frame.iloc[:row_num, :]\n            tail = self.tr_frame.iloc[-row_num:, :]\n            self.tr_frame = concat((head, tail))\n        else:\n            row_num = cast(int, self.max_rows)\n            self.tr_frame = self.tr_frame.iloc[:row_num, :]\n        self.tr_row_num = row_num\n\n    def _get_index_un_strcols(self) -> list[list[str]]:\n        strcols: list[list[str]] = []\n\n        if not is_list_like(self.header) and not self.header:\n            for i, c in enumerate(self.tr_frame):\n                fmt_values = self.collumn_metadata(i)\n                fmt_values = _make_fixed_width(\n                    strings=fmt_values,\n                    justify=self.justify,\n                    minimum=int(self.col_space.get(c, 0)),\n                    adj=self.adj,\n                )\n                strcols.append(fmt_values)\n            return strcols\n\n        if is_list_like(self.header):\n            # cast here since can't be bool if is_list_like\n            self.header = cast(List[str], self.header)\n            if len(self.header) != len(self.columns):\n                raise ValueError(\n                    f\"Writing {len(self.columns)} cols \"\n                    f\"but got {len(self.header)} aliases\"\n                )\n            str_columns = [[label] for label in self.header]\n        else:\n            str_columns = self._get_column_labels_metadata_type(self.tr_frame)\n\n        if self.display_row_sequence_name:\n            for x in str_columns:\n                x.append(\"\")\n\n        for i, c in enumerate(self.tr_frame):\n            cheader = str_columns[i]\n            header_colwidth = max(\n                int(self.col_space.get(c, 0)), *(self.adj.len(x) for x in cheader)\n            )\n            fmt_values = self.collumn_metadata(i)\n            fmt_values = _make_fixed_width(\n                fmt_values, self.justify, minimum=header_colwidth, adj=self.adj\n            )\n\n            max_len = max(max(self.adj.len(x) for x in fmt_values), header_colwidth)\n            cheader = self.adj.justify(cheader, max_len, mode=self.justify)\n            strcols.append(cheader + fmt_values)\n\n        return strcols\n\n    def collumn_metadata(self, i: int) -> list[str]:\n        frame = self.tr_frame\n        formatter = self._get_metadata_type(i)\n        return format_array(\n            frame.iloc[:, i]._values,\n            formatter,\n            float_format=self.float_format,\n            na_rep=self.na_rep,\n            space=self.col_space.get(frame.columns[i]),\n            decimal=self.decimal,\n            leading_space=self.index,\n        )\n\n    def _get_metadata_type(self, i: str | int) -> Callable | None:\n        if isinstance(self.formatters, (list, tuple)):\n            if is_integer(i):\n                i = cast(int, i)\n                return self.formatters[i]\n            else:\n                return None\n        else:\n            if is_integer(i) and i not in self.columns:\n                i = self.columns[i]\n            return self.formatters.get(i, None)\n\n    def _get_column_labels_metadata_type(self, frame: DataFrame) -> list[list[str]]:\n        from pandas.core.indexes.multi import sparsify_labels\n\n        columns = frame.columns\n\n        if isinstance(columns, MultiIndex):\n            fmt_columns = columns.format(sparsify=False, adjoin=False)\n            fmt_columns = list(zip(*fmt_columns))\n            dtypes = self.frame.dtypes._values\n\n            # if we have a Float level, they don't use leading space at all\n            restrict_formatting = any(level.is_floating for level in columns.levels)\n            need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))\n\n            def void_arrangement(x, y):\n                if (\n                    y not in self.formatters\n                    and need_leadsp[x]\n                    and not restrict_formatting\n                ):\n                    return \" \" + y\n                return y\n\n            str_columns = list(\n                zip(*([void_arrangement(x, y) for y in x] for x in fmt_columns))\n            )\n            if self.sparsify and len(str_columns):\n                str_columns = sparsify_labels(str_columns)\n\n            str_columns = [list(x) for x in zip(*str_columns)]\n        else:\n            fmt_columns = columns.format()\n            dtypes = self.frame.dtypes\n            need_leadsp = dict(zip(fmt_columns, map(is_numeric_dtype, dtypes)))\n            str_columns = [\n                [\" \" + x if not self._get_metadata_type(i) and need_leadsp[x] else x]\n                for i, x in enumerate(fmt_columns)\n            ]\n        # self.str_columns = str_columns\n        return str_columns\n\n    def _get_index_metadata_type(self, frame: DataFrame) -> list[str]:\n        # Note: this is only used by to_string() and to_latex(), not by\n        # to_html(). so safe to cast col_space here.\n        col_space = {k: cast(int, v) for k, v in self.col_space.items()}\n        index = frame.index\n        columns = frame.columns\n        fmt = self._get_metadata_type(\"__index__\")\n\n        if isinstance(index, MultiIndex):\n            fmt_index = index.format(\n                sparsify=self.sparsify,\n                adjoin=False,\n                names=self.display_row_sequence_name,\n                formatter=fmt,\n            )\n        else:\n            fmt_index = [index.format(name=self.display_row_sequence_name, formatter=fmt)]\n\n        fmt_index = [\n            tuple(\n                _make_fixed_width(\n                    list(x), justify=\"left\", minimum=col_space.get(\"\", 0), adj=self.adj\n                )\n            )\n            for x in fmt_index\n        ]\n\n        adjoined = self.adj.adjoin(1, *fmt_index).split(\"\\n\")\n\n        # empty space for columns\n        if self.display_col_sequence_name:\n            col_header = [str(x) for x in self._get_column_names()]\n        else:\n            col_header = [\"\"] * columns.nlevels\n\n        if self.header:\n            return col_header + adjoined\n        else:\n            return adjoined\n\n    def _get_column_names(self) -> list[str]:\n        names: list[str] = []\n        columns = self.frame.columns\n        if isinstance(columns, MultiIndex):\n            names.extend(\"\" if name is None else name for name in columns.names)\n        else:\n            names.append(\"\" if columns.name is None else columns.name)\n        return names\n#--------------------------------------------------------------------------------------------------------------------------------------------\nclass InteloraRecordGenerator:\n\n    def __init__(self, fmt: InteloraRecordHandler):\n        self.fmt = fmt\n\n    def iord_clay_format(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        column_format: str | None = None,\n        longtable: bool = False,\n        encoding: str | None = None,\n        multicolumn: bool = False,\n        multicolumn_format: str | None = None,\n        multirow: bool = False,\n        caption: str | None = None,\n        label: str | None = None,\n        position: str | None = None,\n    ) -> str | None:\n        \"\"\"\n        Render a DataFrame to a LaTeX tabular/longtable environment output.\n        \"\"\"\n        from pandas.io.formats.latex import LatexFormatter\n\n        latex_formatter = LatexFormatter(\n            self.fmt,\n            longtable=longtable,\n            column_format=column_format,\n            multicolumn=multicolumn,\n            multicolumn_format=multicolumn_format,\n            multirow=multirow,\n            caption=caption,\n            label=label,\n            position=position,\n        )\n        string = latex_formatter.to_string()\n        return save_to_buffer(string, buf=buf, encoding=encoding)\n\n    def iord_web_format(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        encoding: str | None = None,\n        classes: str | list | tuple | None = None,\n        notebook: bool = False,\n        border: int | None = None,\n        table_id: str | None = None,\n        render_links: bool = False,\n    ) -> str | None:\n        \n        from pandas.io.formats.html import (\n            HTMLFormatter,\n            NotebookFormatter,\n        )\n\n        Klass = NotebookFormatter if notebook else HTMLFormatter\n\n        html_formatter = Klass(\n            self.fmt,\n            classes=classes,\n            border=border,\n            table_id=table_id,\n            render_links=render_links,\n        )\n        string = html_formatter.to_string()\n        return save_to_buffer(string, buf=buf, encoding=encoding)\n\n    def string_formlizer(\n        self,\n        buf: FilePath | WriteBuffer[str] | None = None,\n        encoding: str | None = None,\n        line_width: int | None = None,\n    ) -> str | None:\n        \n        from pandas.io.formats.string import StringFormatter\n\n        string_formatter = StringFormatter(self.fmt, line_width=line_width)\n        string = string_formatter.to_string()\n        return save_to_buffer(string, buf=buf, encoding=encoding)\n\n    def csv_generator(\n        self,\n        path_or_buf: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        encoding: str | None = None,\n        sep: str = \",\",\n        columns: Sequence[Hashable] | None = None,\n        index_label: IndexLabel | None = None,\n        mode: str = \"w\",\n        compression: CompressionOptions = \"infer\",\n        quoting: int | None = None,\n        quotechar: str = '\"',\n        line_terminator: str | None = None,\n        chunksize: int | None = None,\n        date_format: str | None = None,\n        doublequote: bool = True,\n        escapechar: str | None = None,\n        errors: str = \"strict\",\n        storage_options: StorageOptions = None,\n    ) -> str | None:\n        \"\"\"\n        Render dataframe as comma-separated file.\n        \"\"\"\n        from pandas.io.formats.csvs import CSVFormatter\n\n        if path_or_buf is None:\n            created_buffer = True\n            path_or_buf = StringIO()\n        else:\n            created_buffer = False\n\n        csv_formatter = CSVFormatter(\n            path_or_buf=path_or_buf,\n            line_terminator=line_terminator,\n            sep=sep,\n            encoding=encoding,\n            errors=errors,\n            compression=compression,\n            quoting=quoting,\n            cols=columns,\n            index_label=index_label,\n            mode=mode,\n            chunksize=chunksize,\n            quotechar=quotechar,\n            date_format=date_format,\n            doublequote=doublequote,\n            escapechar=escapechar,\n            storage_options=storage_options,\n            formatter=self.fmt,\n        )\n        csv_formatter.save()\n\n        if created_buffer:\n            assert isinstance(path_or_buf, StringIO)\n            content = path_or_buf.getvalue()\n            path_or_buf.close()\n            return content\n\n        return None\n#-------------------------------------------------------------------------------------------------------------------------------------------\ndef prepare_output(\n        self,\n        path_or_buf: FilePath | WriteBuffer[bytes] | WriteBuffer[str] | None = None,\n        sep: str = \",\",\n        na_rep: str = \"\",\n        float_format: str | None = None,\n        columns: Sequence[Hashable] | None = None,\n        header: bool_t | list[str] = True,\n        index: bool_t = True,\n        index_label: IndexLabel | None = None,\n        mode: str = \"w\",\n        encoding: str | None = None,\n        compression: CompressionOptions = \"infer\",\n        quoting: int | None = None,\n        quotechar: str = '\"',\n        line_terminator: str | None = None,\n        chunksize: int | None = None,\n        date_format: str | None = None,\n        doublequote: bool_t = True,\n        escapechar: str | None = None,\n        decimal: str = \".\",\n        errors: str = \"strict\",\n        storage_options: StorageOptions = None,\n    ) -> str | None:\n    \n        df = self if isinstance(self, ABCDataFrame) else self.to_frame()\n\n        formatter = InteloraRecordHandler(\n            frame=df,\n            header=header,\n            index=index,\n            na_rep=na_rep,\n            float_format=float_format,\n            decimal=decimal,\n        )\n\n        return InteloraRecordGenerator(formatter).csv_generator(\n            path_or_buf,\n            line_terminator=line_terminator,\n            sep=sep,\n            encoding=encoding,\n            errors=errors,\n            compression=compression,\n            quoting=quoting,\n            columns=columns,\n            index_label=index_label,\n            mode=mode,\n            chunksize=chunksize,\n            quotechar=quotechar,\n            date_format=date_format,\n            doublequote=doublequote,\n            escapechar=escapechar,\n            storage_options=storage_options,\n        )"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f4990dab-a259-4c44-9c1a-e2cf0f2734ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# mantain single source data stream and aggregating with table manner\ndef singleSource(filtered_df):\n    #Tone\tPositiveTone\tNegativeTone\tPolarity\tActivityDensity\tSelfDensity\tWordCount\n    filtered_df_result = (filtered_df.groupby(F.date_format(\"DATE\", \"y-MM-dd\").alias(\"date\"), 'SourceCommonName')\n                                      .agg(F.mean(\"Tone\"), F.mean(\"PositiveTone\"), F.mean(\"NegativeTone\"), \n                                           F.mean(\"Polarity\"), F.mean(\"ActivityDensity\"), \n                                           F.mean(\"SelfDensity\"), F.mean(\"WordCount\"))\n                                      .withColumn(\"date\", F.to_date(\"date\", format=\"y-MM-dd\"))\n                                      .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n                                      .orderBy(F.col(\"date\").asc())\n                          )\n    filtered_df_result = (filtered_df_result.select(\"date\", \"SourceCommonName\", F.col(\"avg(Tone)\").alias(\"Tone\"), F.col(\"avg(PositiveTone)\").alias(\"PositiveTone\"), \n                                             F.col(\"avg(NegativeTone)\").alias(\"NegativeTone\"),\n                                             F.col(\"avg(Polarity)\").alias(\"Polarity\"), F.col(\"avg(ActivityDensity)\").alias(\"ActivityDensity\"), \n                                             F.col(\"avg(SelfDensity)\").alias(\"SelfDensity\"), F.col(\"avg(WordCount)\").alias(\"WordCount\"))\n                                           \n                          )\n    return filtered_df_result\n\n# calculate average tone data\ndef avg_day_tone(filtered_df, name):\n    \"\"\" \"\"\"\n    colname = f\"{name.replace(' ', '_')}_tone\"\n    tone_df = (filtered_df.groupby(date_alignment(\"DATE\", \"y-MM-dd\").alias(\"date\"))\n                          .agg((F.sum(\"Tone\") / F.sum(\"WordCount\")).alias(colname))\n                          .select(\"date\", f\"{colname}\")\n                          .withColumn(\"date\", beginning_date(\"date\", format=\"y-MM-dd\"))\n                          .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n                          .orderBy(F.col(\"date\").asc())\n              )\n    return tone_df\n\n# call in make_tables \n# def subtract_cols(df, col1, col2):                                              \n#     df = (df.withColumn(col1, df[f\"{col1}\"] - df[f\"{col2}\"])\n#             .withColumnRenamed(col1, col1.replace(\"_tone\", \"_diff\")))\n#     return df\n\n# getting column filter with date grouping\ndef get_col_avgs(df):\n    exclude = [k for k, v in df.dtypes if v in [\"date\", \"timestamp\", \"string\", \"SourceCommonName\"]]\n    avgs = df.select([F.avg(c).alias(c) for c in df.columns if c not in exclude]).collect()[0]\n    return {c: avgs[c] for c in df.columns if c not in exclude}\n\n# # COMMAND ----------\n\n# def show_df_rounded(df, places=4, rows=20):\n#     dtypes = {k: v for k, v in df.dtypes}\n#     date_cols = [k for k, v in dtypes.items() if v in [\"date\", \"timestamp\"]]\n#     str_cols = [k for k, v in dtypes.items() if v == \"string\"]\n#     int_cols = [k for k, v in dtypes.items() if \"int\" in v]\n    \n#     show_cols = [F.date_format(c, \"y-MM-dd\").alias(c) if c in date_cols\n#                  else (F.col(c).alias(c) if c in str_cols\n#                  else (F.format_number(c, 0).alias(c) if c in int_cols\n#                  else (F.format_number(c, places).alias(c))))\n#                  for c in df.columns]\n#     show_cols = [c for c in show_cols]\n#     df.select(*show_cols).limit(rows).show()\n\n# # DBTITLE 1,Load Data from Delta Table\n# def load_data(save_path, file_name): \n#   df = (spark.read.format(\"delta\")\n#                       .option(\"header\", \"true\")\n#                       .option(\"inferSchema\", \"true\")\n#                       .load(os.path.join(save_path, file_name))\n#            )\n#   return df.toPandas()\n\n# function to filter un wanted data\ndef filter_non_esg(df): \n    return df[(df['E']==True) | (df['S'] == True) | (df['G'] == True)]\n\n# COMMAND ----------\n\n# web based graph display data generator\nclass graph_creator:\n    def __init__(self, df):\n        self.df = df\n\n    def create_graph(self):\n        # Find Edges\n        df_edge = pd.DataFrame(self.df.groupby(\"URL\").Organization.apply(list)\n                               ).reset_index()\n\n        get_tpls = lambda r: (list(itertools.combinations(r, 2)) if\n                              len(r) > 1 else None)\n        df_edge[\"SourceDest\"] = df_edge.Organization.apply(get_tpls)\n        df_edge = df_edge.intelora_data_aligner(\"SourceDest\").to_clean_require(subset=[\"SourceDest\"])\n\n        # Get Weights\n        source_dest = pd.DataFrame(df_edge.SourceDest.tolist(),\n                                   columns=[\"Source\", \"Dest\"])\n        sd_mapping = source_dest.intelora.arranger([\"Source\", \"Dest\"]).size()\n        get_weight = lambda r: sd_mapping[r.Source, r.Dest]\n        source_dest[\"weight\"] = source_dest.intelora_add(get_weight, axis=1)\n\n        # Get\n        self.organizations = set(source_dest.Source.unique()).union(\n                             set(source_dest.Dest.unique()))\n        self.G = nx.from_pandas_edgelist(source_dest, source=\"Source\",\n            target=\"Dest\", edge_attr=\"weight\", create_using=nx.Graph)\n        return self.G\n\n# # COMMAND ----------\n\n# function to fit graph with core data\ndef get_embeddings(G, organizations):\n    # Fit graph\n    g2v = NVVV()\n    g2v.fit(G)\n    \n    # Embeddings\n    embeddings = g2v.model.wv.vectors\n    pca = PCA(n_components=3)\n    principalComponents = pca.fit_transform(embeddings)\n    d_e = pd.DataFrame(principalComponents)\n    d_e[\"company\"] = organizations\n    return d_e, g2v\n\n# COMMAND ----------\n\n# simple connection bridge function\ndef get_connections(organizations, topn=25):\n    l = [g2v.model.wv.most_similar(org, topn=topn)\n         for org in organizations]\n    df_sim = pd.DataFrame(l, columns=[f\"n{i}\" for i in range(topn)])\n    for col in df_sim.columns:\n        new_cols = [f\"{col}_rec\", f\"{col}_conf\"]\n        df_sim[new_cols] = pd.DataFrame(df_sim[col].tolist(), \n                                        index=df_sim.index)\n    df_sim = df_sim.drop(columns=[f\"n{i}\" for i in range(topn)])\n    df_sim.insert(0, \"company\", list(organizations))\n    return df_sim\n\n# # COMMAND ----------\n\n# connecting with gdelt and then embedating data \ndef process_corealgo_and_databrich(start_date,end_date): \n    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_Russell_top_300\"\n    save_dir = os.path.join(base_dir, f\"{start}__to__{end}\")\n    csv_file = \"data_as_csv.csv\"\n\n    # Load data\n    print(\"Loading Data\")\n    df = pd.read_csv(os.path.join(save_dir, csv_file).replace(\"dbfs:/\", \"/dbfs/\"))\n    df = filter_non_esg(df)\n    # Create graph\n    print(\"Creating Graph\")\n    creator = graph_creator(df)\n    G = creator.create_graph()\n    organizations = list(creator.organizations)\n\n    # Save graph as pkl\n    fp = os.path.join(save_dir, \"organization_graph.pkl\").replace(\"dbfs:/\", \"/dbfs/\")\n    with open(fp, \"wb\") as f:\n        pickle.dump(G, f)\n        \n    # Create embeddings\n    print(\"Creating embeddings\")\n    emb_path = os.path.join(save_dir, \"pca_embeddings.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n    d_e, g2v = get_embeddings(G, organizations)\n    d_e.to_csv(emb_path, index=False)\n    \n    # Create connections\n    print(\"Creating connections\")\n    df_sim = get_connections(organizations)\n    sim_path = os.path.join(save_dir, \"connections.csv\")\n    df_sim.to_csv(sim_path.replace(\"dbfs:/\", \"/dbfs/\"))\n    \n    # Save organizations as delta\n    conn_path = os.path.join(save_dir, \"CONNECTIONS\")\n    conn_data = spark.createDataFrame(df_sim)\n    conn_data.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(conn_path)\n    \n# def make_tables(start_date, end_date):\n#      \"\"\"\n#      \"\"\"\n#      # Directories\n#     org_types = f\"Russell_top_{Fields.n_orgs}\"\n#     base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_{org_types}\"\n#     range_save_dir = os.path.join(base_dir, f\"{start_date}__to__{end_date}\")\n#     esg_dir = os.path.join(range_save_dir, \"esg_scores\")\n#     dbutils.fs.mkdirs(esg_dir)\n    \n#      # Load data\n#     data_path = os.path.join(range_save_dir, \"data_as_delta\")\n#     try:\n#         data = (spark.read.format(\"delta\")\n#                       .option(\"header\", \"true\")\n#                       .option(\"inferSchema\", \"true\")\n#                       .load(data_path)\n#                 )\n#          print(\"Data Loaded!\")\n#     except:\n#          print(\"Data for these dates hasn't been generated!!!\")\n#         return\n\n#     # Get all organizations\n#     print(\"Finding all Organizations\")\n#     organizations = [x.Organization for x in data.select(\n#                      \"Organization\").distinct().collect()]\n    \n#     # Get the overall tone\n#     print(\"Calculating Tones Over Time\")\n#     overall_tone = daily_tone(data, \"industry\")\n#     esg_tones = {L: daily_tone(data.filter(f\"{L} == True\"), \"industry\")\n#                  for L in [\"E\", \"S\", \"G\"]}\n    \n#     # Loop through the organizations to get the average daily tone for each company\n#     pct_idxs = range(0, len(organizations), len(organizations) // 10)\n#     for i, org in enumerate(organizations):\n#         if i in pct_idxs:\n#             print(f\"{pct_idxs.index(i) * 10}%\")\n#         tone_label = f\"{org.replace(' ', '_')}_tone\"\n        \n#         overall_org_df = data.filter(f\"Organization == '{org}'\")\n#         org_tone = daily_tone(overall_org_df, org)\n#         overall_tone = subtract_cols(overall_tone.join(org_tone, on=\"date\", how=\"left\"),\n#                                      tone_label, \"industry_tone\")\n      \n#         for L, tdf in esg_tones.items():\n#             esg_org_df = overall_org_df.filter(f\"{L} == True\")\n#             esg_org_tone = daily_tone(esg_org_df, org)\n#             esg_tones[L] = subtract_cols(tdf.join(esg_org_tone, on=\"date\", how=\"left\"), \n#                                          tone_label, \"industry_tone\")            \n#     del data   \n    \n#     # Average to get overall scores\n#     print(\"Computing Overall Scores\")\n#     scores = {}\n#     overall_scores = get_col_avgs(overall_tone)\n#     esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n\n#     for org in organizations:\n#         diff_label = f\"{org.replace(' ', '_')}_diff\"\n#         scores[org] = {L: tdf[diff_label] for L, tdf in esg_scores.items()}\n#         scores[org][\"T\"] = overall_scores[diff_label]\n      \n      \n#     # Save all the tables\n#     print(\"Saving Tables\")  \n    \n#     # Overall ESG\n#     print(\"    Daily Overall ESG\")\n#     path = os.path.join(esg_dir, \"overall_daily_esg_scores.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n#     pd_df = overall_tone.toPandas().set_index(\"date\").sort_index().asfreq(freq=\"D\", method=\"ffill\")\n#     pd_df.to_csv(path, index=True)\n    \n#     # E, S, and G\n#     for L, tdf in esg_tones.items():\n#         print(\"    Daily \" + L)\n#         path = os.path.join(esg_dir, f\"daily_{L}_score.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n#         pd_df = tdf.toPandas().set_index(\"date\").sort_index().asfreq(freq=\"D\", method=\"ffill\")\n#         pd_df.to_csv(path, index=True)\n\n#     # Averaged scores\n#     print(\"    Average Scores\")\n#     score_path = path = os.path.join(esg_dir, \"average_esg_scores.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n#     pd.DataFrame(scores).to_csv(score_path, index=True)\n#     print(\"DONE!\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"deec22cd-7448-4f8e-a74c-d966fc7e21bb"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["def calculating_esg_values(start_date, end_date):\n    \"\"\"\n    \"\"\"\n    print(\"calulation started\")\n    data_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\"\n    try:\n        data = (spark.read.format(\"delta\")\n                     .option(\"header\", \"true\")\n                     .option(\"inferSchema\", \"true\")\n                     .load(data_path)\n               )\n        print(\"Data Loaded!\")\n    except:\n        print(\"Data for these dates hasn't been generated!!!\")\n        return\n\n    # Get all organizations\n    print(\"Finding all Organizations\")\n    organizations = [x.Organization for x in data.select(\n                     \"Organization\").distinct().collect()]\n    \n    # Get the overall tone\n    print(organizations)\n    print(\"Calculating Tones Over Time\")\n    overall_tone = avg_day_tone(singleSource(data), \"industry\")                                              \n    esg_tones = {L: avg_day_tone(singleSource(data.filter(f\"{L} == True\")), \"industry\")\n                 for L in [\"E\", \"S\", \"G\"]}\n    \n    pct_idxs = range(0, len(organizations))\n    \n    for i, org in enumerate(organizations):\n        tone_label = f\"{org.replace(' ', '_')}_tone\"\n\n        overall_org_df = data.filter(f\"Organization == '{org}'\")\n        org_tone = avg_day_tone(singleSource(overall_org_df), org)\n\n        overall_tone = overall_tone.join(org_tone, on=\"date\", how=\"left\")\n      \n        for L, tdf in esg_tones.items():\n            esg_org_df = overall_org_df.filter(f\"{L} == True\")\n            esg_org_tone = avg_day_tone(singleSource(esg_org_df), org)\n            \n            esg_tones[L] = tdf.join(esg_org_tone, on=\"date\", how=\"left\")\n    \n    del data \n    \n    # get tone data for pre processing\n    shouls_keep_garbaze = \"True\"\n    if shouls_keep_garbaze == \"false\": \n        make_embeddings_and_connections(start_date,end_date)\n    else:\n        # Average to get overall scores\n        print(organizations)\n        print(\"Computing Overall Scores\")\n        scores = {}\n        print(\"    Calculating overall tone\")\n        overall_scores = get_col_avgs(overall_tone)\n        print(\"    Calculating esg tone\")\n        esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n    \n        #print(overall_scores)\n        print(esg_scores)\n        print(\"DONE!\")\n        return esg_scores, overall_tone\n        "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b260a08a-b986-4300-b39a-4a0bb72d5cd1"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Begin process to download data for calculation\ndef download_company_data(base_dir, start_date, end_date, org_name):\n    #print(org_name)\n    _ = download_gdelt_data(start_date, end_date,org_name, save_csv=True)\n    #print(org_name)\n    esg_score1, _ = calculating_esg_values(start_date, end_date)\n    return esg_score1"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"06496dbe-0dad-4ceb-b16c-8340047e555a"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\"></div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\"></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Calculating ESG Scores for input Companies:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3bbb3b2c-cc98-402b-946f-4c331f6e105c"}}},{"cell_type":"code","source":["# web REST module to take input\nbase_dir = \"dbfs:/mnt/esg/financial_report_data\" \ndbutils.widgets.text(\"myinput\",\"microsoft;apple\")\ndbutils.widgets.text(\"startdate\",\"2022-05-01\")\ndbutils.widgets.text(\"enddate\",\"2022-05-02\")\n\nvar_a = dbutils.widgets.get(\"myinput\")\nvar_a = var_a.split(';')\nprint(var_a)\nstart_date = dbutils.widgets.get(\"startdate\")\nend_date = dbutils.widgets.get(\"enddate\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a1f7576-20f7-48bd-b784-c87c45eb7a7b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">[&#39;microsoft&#39;, &#39;apple&#39;]\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">[&#39;microsoft&#39;, &#39;apple&#39;]\n</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# manage company data during date period\noutput = download_company_data(base_dir, start_date, end_date, var_a)\ndbutils.notebook.exit(output)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"61e8c5fc-2dfe-4918-8917-34243d699e40"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"type":"exit","data":"{'E': {'industry_tone': -0.00037340860057307433, 'microsoft_tone': -0.00042677988406399217, 'apple_tone': 0.002173904392291133}, 'S': {'industry_tone': 0.0001253949109221482, 'microsoft_tone': 0.0004051838667690962, 'apple_tone': -0.0009927997403526272}, 'G': {'industry_tone': -0.0006220543071157147, 'microsoft_tone': -0.0007719906056858187, 'apple_tone': -2.187044036061831e-05}}","arguments":{},"metadata":{}}},"output_type":"display_data","data":{"text/plain":["{'E': {'industry_tone': -0.00037340860057307433, 'microsoft_tone': -0.00042677988406399217, 'apple_tone': 0.002173904392291133}, 'S': {'industry_tone': 0.0001253949109221482, 'microsoft_tone': 0.0004051838667690962, 'apple_tone': -0.0009927997403526272}, 'G': {'industry_tone': -0.0006220543071157147, 'microsoft_tone': -0.0007719906056858187, 'apple_tone': -2.187044036061831e-05}}"]}}],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9590e05e-fc1f-4335-ae5b-c4b4da5d16f6"}},"outputs":[],"execution_count":0}],"metadata":{"kernelspec":{"display_name":"Python 3.9.7 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython3","codemirror_mode":{"name":"ipython","version":3},"version":"3.9.7","nbconvert_exporter":"python","file_extension":".py"},"vscode":{"interpreter":{"hash":"bf9f081df7a9fb6e3e60ce586fd6298abdfbef8d0b52c7e06ae6a4386281fc66"}},"application/vnd.databricks.v1+notebook":{"notebookName":"esg_final_cleaned_latest","dashboards":[],"notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":-1,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"language":"python","widgets":{"enddate":{"nuid":"952f80be-0341-4f67-911b-08855fa315b7","currentValue":"2022-05-05","widgetInfo":{"widgetType":"text","name":"enddate","defaultValue":"2022-05-02","label":null,"options":{"widgetType":"text","validationRegex":null}}},"myinput":{"nuid":"d665e895-3192-4cd3-bdca-caf8ac4bc19b","currentValue":"microsoft;apple","widgetInfo":{"widgetType":"text","name":"myinput","defaultValue":"microsoft;apple","label":null,"options":{"widgetType":"text","validationRegex":null}}},"startdate":{"nuid":"fcdcf192-5ee6-41ab-bf7d-6d10630f4034","currentValue":"2022-05-04","widgetInfo":{"widgetType":"text","name":"startdate","defaultValue":"2022-05-01","label":null,"options":{"widgetType":"text","validationRegex":null}}}},"notebookOrigID":1148977267166212}},"nbformat":4,"nbformat_minor":0}
