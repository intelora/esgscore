{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame as spark_DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from itertools import product\n",
    "import time\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "from pyspark.sql import Window\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def also_call_as(c):\n",
    "    \"\"\"\n",
    "    Make a list of aliases for a given company using a few predefined\n",
    "    rules. Try to encompass as many options as possible keeping in mind\n",
    "    there will be aliases left out.\n",
    "    \"\"\"\n",
    "    c = c.lower()\n",
    "\n",
    "    # URLs (e.g. \".com\")\n",
    "    if len(c) > 3 and c[-4] == \".\":\n",
    "        a = c\n",
    "        c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n",
    "        aliases = set([c, a])\n",
    "    else:\n",
    "        aliases = set([c])\n",
    "\n",
    "    # Single letter endings\n",
    "    if len(c.split()[-1]) == 1:\n",
    "        c = c.rsplit(\" \", 1)[0]\n",
    "        aliases.add(c)\n",
    "\n",
    "    # Company legal endings\n",
    "    \n",
    "    endings = [\"inc\",\n",
    "               \"corp\",\n",
    "               \"plc\", \n",
    "               \"reit\", \n",
    "               \"co\", \n",
    "               \"cor\", \n",
    "               \"group\", \n",
    "               \"company\",\n",
    "               \"trust\",\n",
    "               \"energy\",\n",
    "               \"international\",\n",
    "               \"of america\",\n",
    "               \"pharmaceuticals\",\n",
    "               \"clas\",\n",
    "               \"in\", \"nv\",\n",
    "               \"sa\", \n",
    "               \"re\", \n",
    "               \"pvt ltd\",\n",
    "               \"private limited\" ,\n",
    "               \"india private limited\"\n",
    "               \"Co\",\n",
    "               \"CO.\",\n",
    "               \"Companies\",\n",
    "               \"Company\",\n",
    "               \"Corp\",\n",
    "               \"CORP.\",\n",
    "               \"Corporation\",\n",
    "               \"Inc\",\n",
    "               \"INC.\",\n",
    "               \"Incorporated\",\n",
    "               \"Limited\",\n",
    "               \"Ltd\",\n",
    "               \"Professional Corporation\",\n",
    "               \"Chartered\",\n",
    "               \"Limited\",\n",
    "               \"Ltd\",\n",
    "               \"Ltd.\",\n",
    "               \"pa\",\n",
    "               \"p.c \",\n",
    "               \"Professional Association\",\n",
    "               \"Professional Corporation\",\n",
    "               \"Professional Service Corporation\",\n",
    "               \"psc\",\n",
    "               \"sc.\",\n",
    "               \"Service Corporation\"]\n",
    "    \n",
    "    n_endings = 3  # Can have up to 3 of these endings\n",
    "    for _ in range(n_endings):\n",
    "        aliases.update([a.rsplit(\" \", 1)[0] for a in aliases if\n",
    "                        any([a.endswith(\" \" + e) for e in endings])])\n",
    "        c = c.rsplit(\" \", 1)[0] if any([c.endswith(\" \" + e) for e in endings]) else c\n",
    "\n",
    "    # Alias any dashes and replace in company name\n",
    "    aliases.update([a.replace(\"-\", \"\") for a in aliases] +\n",
    "                   [a.replace(\"-\", \" \") for a in aliases])\n",
    "    c = c.replace(\"-\", \" \")\n",
    "\n",
    "    # If '&' stands on its own, add alias of 'and'\n",
    "    aliases.update([a.replace(\" & \", \" and \") for a in aliases])\n",
    "\n",
    "    return {c: list(aliases)}\n",
    "\n",
    "\n",
    "def generate_common_org_names(companies):\n",
    "    \"\"\"\n",
    "    Download the companies and loop through them to find their aliases\n",
    "    \"\"\"\n",
    "    #companies = [org_name]\n",
    "    comp_dict = dict()\n",
    "    for c in companies:\n",
    "        comp_dict.update(also_call_as(c))\n",
    "    return comp_dict\n",
    "\n",
    "\n",
    "combine_spark_dfs = lambda sdf_list: reduce(spark_DataFrame.union, sdf_list)\n",
    "\n",
    "class Common_MetaData:\n",
    "    \"\"\" Variables to use across many functions. \"\"\"\n",
    "    keep = [\"DATE\",\n",
    "            \"SourceCommonName\", \n",
    "            \"DocumentIdentifier\", \n",
    "            \"Themes\",\n",
    "            \"Organizations\",\n",
    "            \"V2Tone\"]\n",
    "    tone = [\"Tone\",\n",
    "            \"PositiveTone\",\n",
    "            \"NegativeTone\", \n",
    "            \"Polarity\",\n",
    "            \"ActivityDensity\", \n",
    "            \"SelfDensity\",\n",
    "            \"WordCount\"]\n",
    "    organizations = None\n",
    "\n",
    "\n",
    "\n",
    "@udf(ArrayType(StringType(), True))\n",
    "def simple_expand_spark(x):\n",
    "    \"\"\" Expand a semicolon separated strint to a list (ignoring empties)\"\"\"\n",
    "    if not x:\n",
    "        return []\n",
    "    return list(filter(None, x.split(\";\")))\n",
    "\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), DoubleType()))\n",
    "def tone_expand_spark(x):\n",
    "    \"\"\" Expand the tone field. \"\"\"\n",
    "    if not x:\n",
    "        return {t: None for t in Common_MetaData.tone}\n",
    "    return {Common_MetaData.tone[i]: float(v) for i, v in enumerate(x.split(\",\"))}\n",
    "\n",
    "\n",
    "\n",
    "@udf(BooleanType())\n",
    "def has_theme_spark(x, theme):\n",
    "    \"\"\" Is the given theme included in any of the listed themes? \"\"\"\n",
    "    return any([theme in lst.split(\"_\") for lst in x])\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def clean_organization(s):\n",
    "    \"\"\" Standardize the organization names. \"\"\"\n",
    "    for k, v in Common_MetaData.organizations.items():\n",
    "        if v[0] in s.split() :\n",
    "            return k\n",
    "    return s.lower()\n",
    "\n",
    "\n",
    "def also_call_create(c):\n",
    "    \"\"\"\n",
    "    Make a list of aliases for a given company using a few predefined\n",
    "    rules. Try to encompass as many options as possible keeping in mind\n",
    "    there will be aliases left out.\n",
    "    \"\"\"\n",
    "    c = c.lower()\n",
    "\n",
    "    # URLs (e.g. \".com\")\n",
    "    if len(c) > 3 and c[-4] == \".\":\n",
    "        a = c\n",
    "        c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n",
    "        aliases = set([c, a])\n",
    "    else:\n",
    "        aliases = set([c])\n",
    "\n",
    "    return aliases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading , Filtering and Storing Gdelt Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ed691b2-9044-4bd4-ba48-3d72ef2052d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Redesign_data_format:\n",
    "    def redesign_sdf(self, sdf, file_path_refined, org_name):\n",
    "        \"\"\"\n",
    "        Given a spark data frame of the downloaded data, reformat it\n",
    "        into human-readable Common_MetaData.\n",
    "        Add a few more Common_MetaData for our purposes.\n",
    "        \"\"\"\n",
    "        setattr(Common_MetaData, \"organizations\",generate_common_org_names(org_name))\n",
    "        \n",
    "        sdf = sdf.select(*Common_MetaData.keep)\n",
    "        \n",
    "        if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            # Reformat existing columns\n",
    "            sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n",
    "                      .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n",
    "                      .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n",
    "                      .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n",
    "                   )\n",
    "\n",
    "            # Create ESG columns & explode organization column\n",
    "            sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n",
    "                      .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n",
    "                      .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n",
    "                   )\n",
    "        \n",
    "            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n",
    "            print(\"sdf created\")\n",
    "        \n",
    "        else:\n",
    "            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n",
    "        \n",
    "        sdf = (sdf.withColumn(\"Organization\", F.explode(\"Organizations\"))\n",
    "                  .withColumn(\"Organization\", clean_organization(\"organization\"))\n",
    "                  .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n",
    "              )\n",
    "\n",
    "        # Expand tone columns\n",
    "        exprs = [F.col(\"V2Tone\").getItem(k).alias(k) for k in Common_MetaData.tone]\n",
    "\n",
    "        sdf = sdf.select(*sdf.columns, *exprs).drop(\"V2Tone\")\n",
    "        #print(sdf.column)\n",
    "        return sdf\n",
    "\n",
    "\n",
    "    def download_and_generate_gdelt_table1(self, date, gd):\n",
    "        \"\"\"\n",
    "        Download the GDELT table as a pandas dataframe using the gdelt package.\n",
    "        Return a spark data frame.\n",
    "        \"\"\"\n",
    "        pdf = gd.Search([date], table=\"gkg\",coverage=True, output=\"df\")\n",
    "        pdf[\"DATE\"] = pd.to_datetime(pdf[\"DATE\"], format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        sdf = spark.createDataFrame(pdf)\n",
    "        print(\"   * loaded *  \", date)\n",
    "        return sdf\n",
    "    \n",
    "    def download_and_generate_gdelt_table(self, date, file_path):\n",
    "        file_path_os =  file_path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "        s = []\n",
    "        for i in range(24):\n",
    "            for j in list(range(0, 60, 15)):\n",
    "                if i<10:\n",
    "                    if j<10:\n",
    "                        s.append('0' + str(i) + '0' + str(j))\n",
    "                    else:\n",
    "                        s.append('0' + str(i) + str(j))\n",
    "                else:\n",
    "                    if j<10:\n",
    "                        s.append(str(i) + '0' + str(j))\n",
    "                    else:\n",
    "                         s.append(str(i) + str(j))\n",
    "                            \n",
    "        if not os.path.exists(file_path_os):\n",
    "            li = ''.join(date.split('-'))\n",
    "            c=0\n",
    "            df1 = None\n",
    "            for elem in s:\n",
    "                try:\n",
    "                    print(li, elem)\n",
    "                    response = requests.get('http://data.gdeltproject.org/gdeltv2/'+li+ elem + '00.gkg.csv.zip')\n",
    "                    #'http://data.gdeltproject.org/gkg/20220204.gkg.csv.zip')\n",
    "                    buffer = BytesIO(response.content)\n",
    "                   \n",
    "                    frame = pd.read_csv(buffer, compression='zip', sep='\\t',header=None, warn_bad_lines=True,encoding='latin')\n",
    "                    frame[1] = pd.to_datetime(frame[1], format=\"%Y%m%d%H%M%S\")\n",
    "        \n",
    "                    frame.columns = ['GKGRECORDID', \n",
    "                                     'DATE', \n",
    "                                     'SourceCollectionIdentifier', \n",
    "                                     'SourceCommonName',\n",
    "                                     'DocumentIdentifier',\n",
    "                                     'Counts',\n",
    "                                     'V2Counts', \n",
    "                                     'Themes', \n",
    "                                     'V2Themes',\n",
    "                                     'Locations',\n",
    "                                     'V2Locations', \n",
    "                                     'Persons', \n",
    "                                     'V2Persons', \n",
    "                                     'Organizations',\n",
    "                                     'V2Organizations', \n",
    "                                     'V2Tone',\n",
    "                                     'Dates',\n",
    "                                     'GCAM',\n",
    "                                     'SharingImage',\n",
    "                                     'RelatedImages', \n",
    "                                     'SocialImageEmbeds', \n",
    "                                     'SocialVideoEmbeds', \n",
    "                                     'Quotations',\n",
    "                                     'AllNames', \n",
    "                                     'Amounts', \n",
    "                                     'TranslationInfo', \n",
    "                                     'Extras'] \n",
    "                    columns1 = ['DATE', \n",
    "                                'SourceCollectionIdentifier', \n",
    "                                'SourceCommonName',\n",
    "                                'DocumentIdentifier',\n",
    "                                'Counts', \n",
    "                                'V2Counts',\n",
    "                                'Themes',\n",
    "                                'V2Themes',\n",
    "                                'Locations',\n",
    "                                'V2Locations',\n",
    "                                'Organizations',\n",
    "                                'V2Organizations',\n",
    "                                'V2Tone',\n",
    "                                'Dates'] \n",
    "                    frame = frame[columns1]\n",
    "                    print(frame.shape)\n",
    "                    if c==0:\n",
    "                        df1 = frame\n",
    "                        c=1\n",
    "                    else:\n",
    "                        df1 = df1.append(frame, ignore_index=True)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            sdf = spark.createDataFrame(df1)\n",
    "            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path)\n",
    "            print(\"   * loaded *  \", date)\n",
    "        else:\n",
    "            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path)\n",
    "        return sdf\n",
    "\n",
    "\n",
    "    def getting_all_org_data(self, start_date, end_date, main_dir , refined_dir,organisation_name = None, override_save=False):\n",
    "        \"\"\"\n",
    "        For each date between start_date and end_date, either download\n",
    "        and clean the data or load the pre-saved data. Save the day's data\n",
    "        in case of future use (so it doesn't have to be downloaded and cleaned again)\n",
    "        \"\"\"\n",
    "        print(\"Loading and cleaning all data\")\n",
    "        data_list = []\n",
    "\n",
    "        # Download and format the daily data\n",
    "        for i, date in enumerate(pd.date_range(start_date, end_date).astype(str)):\n",
    "            if i % 7 == 1:\n",
    "                # Prevent it hanging like it does sometimes\n",
    "                time.sleep(60)\n",
    "\n",
    "            try:\n",
    "                file_path = os.path.join(main_dir, date)\n",
    "                file_path_refined = os.path.join(refined_dir, date)\n",
    "                df = self.redesign_sdf(self.download_and_generate_gdelt_table(date, file_path), file_path_refined, organisation_name)      \n",
    "                data_list.append(df)\n",
    "                del df\n",
    "                spark.catalog.clearCache()\n",
    "\n",
    "            except Exception as e:\n",
    "              print(f\"!!! Failed to complete {date}!\")\n",
    "              print(\"  ****   Reason:\\n\" + str(e) + \"\\n\\n\")\n",
    "\n",
    "        return combine_spark_dfs(data_list)\n",
    "    def reformat_sdf(self, sdf, file_path_refined, org_name):\n",
    "        \"\"\"\n",
    "        Given a spark data frame of the downloaded data, reformat it\n",
    "        into human-readable Common_MetaData.\n",
    "        Add a few more Common_MetaData for our purposes.\n",
    "        \"\"\"\n",
    "        sdf = sdf.select(*Common_MetaData.keep)\n",
    "        \n",
    "        if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            # Reformat existing columns\n",
    "            sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n",
    "                      .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n",
    "                      .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n",
    "                      .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n",
    "                   )\n",
    "\n",
    "            # Create ESG columns & explode organization column\n",
    "            sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n",
    "                      .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n",
    "                      .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n",
    "                   )\n",
    "        \n",
    "            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n",
    "            print(\"sdf created\")\n",
    "        \n",
    "        else:\n",
    "            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n",
    "        \n",
    "        sdf = (sdf.withColumn(\"Organization\", F.explode(\"Organizations\"))\n",
    "                  .withColumn(\"Organization\", clean_organization(\"organization\"))\n",
    "                  .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n",
    "              )\n",
    "\n",
    "        return sdf\n",
    "    def preprocess_gdelt_data(self ,start_date, end_date, org_name = None, save_csv=True):\n",
    "        \"\"\"\n",
    "        \"\"\"   \n",
    "        dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n",
    "\n",
    "        base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n",
    "        if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            dbutils.fs.mkdirs(base_dir)\n",
    "\n",
    "        org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n",
    "        if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            dbutils.fs.mkdirs(org_dir)\n",
    "\n",
    "        base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n",
    "        base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n",
    "\n",
    "        if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            dbutils.fs.mkdirs(base_data_dir)\n",
    "        if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n",
    "            dbutils.fs.mkdirs(base_data_dir_refined)\n",
    "        # Download and reformat the data\n",
    "        print('')\n",
    "        data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n",
    "        print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n",
    "              f\"organizations from {start_date} to {end_date}\")\n",
    "\n",
    "        # Save the data\n",
    "        print(\"Saving Data...\")\n",
    "        data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n",
    "        if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            dbutils.fs.mkdirs(base_data_dir)\n",
    "        data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n",
    "        print(f\"Saved to {data_save_path}\")\n",
    "\n",
    "        return data\n",
    "\n",
    "\n",
    "\n",
    "    # COMMAND ----------\n",
    "\n",
    "def download_gdelt_data(start_date, end_date, org_name = None, save_csv=True):\n",
    "    \"\"\"\n",
    "    \"\"\"   \n",
    "    dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n",
    "\n",
    "    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n",
    "    if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(base_dir)\n",
    "\n",
    "    org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n",
    "    if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(org_dir)\n",
    "\n",
    "    base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n",
    "    base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n",
    "\n",
    "    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(base_data_dir)\n",
    "    if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n",
    "        dbutils.fs.mkdirs(base_data_dir_refined)\n",
    "    # Download and reformat the data\n",
    "    print('')\n",
    "    data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n",
    "    print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n",
    "          f\"organizations from {start_date} to {end_date}\")\n",
    "\n",
    "    # Save the data\n",
    "    print(\"Saving Data...\")\n",
    "    data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n",
    "    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(base_data_dir)\n",
    "    data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n",
    "    print(f\"Saved to {data_save_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESG Computating Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "deec22cd-7448-4f8e-a74c-d966fc7e21bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def singleSource(filtered_df):\n",
    "    #Tone\tPositiveTone\tNegativeTone\tPolarity\tActivityDensity\tSelfDensity\tWordCount\n",
    "    filtered_df_result = (filtered_df.groupby(F.date_format(\"DATE\", \"y-MM-dd\").alias(\"date\"), 'SourceCommonName')\n",
    "                                      .agg(F.mean(\"Tone\"), F.mean(\"PositiveTone\"), F.mean(\"NegativeTone\"), \n",
    "                                           F.mean(\"Polarity\"), F.mean(\"ActivityDensity\"), \n",
    "                                           F.mean(\"SelfDensity\"), F.mean(\"WordCount\"))\n",
    "                                      .withColumn(\"date\", F.to_date(\"date\", format=\"y-MM-dd\"))\n",
    "                                      .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
    "                                      .orderBy(F.col(\"date\").asc())\n",
    "                          )\n",
    "    filtered_df_result = (filtered_df_result.select(\"date\", \"SourceCommonName\", F.col(\"avg(Tone)\").alias(\"Tone\"), F.col(\"avg(PositiveTone)\").alias(\"PositiveTone\"), \n",
    "                                             F.col(\"avg(NegativeTone)\").alias(\"NegativeTone\"),\n",
    "                                             F.col(\"avg(Polarity)\").alias(\"Polarity\"), F.col(\"avg(ActivityDensity)\").alias(\"ActivityDensity\"), \n",
    "                                             F.col(\"avg(SelfDensity)\").alias(\"SelfDensity\"), F.col(\"avg(WordCount)\").alias(\"WordCount\"))\n",
    "                                           \n",
    "                          )\n",
    "    return filtered_df_result\n",
    "\n",
    "\n",
    "def avg_day_tone(filtered_df, name):\n",
    "    \"\"\" \"\"\"\n",
    "    colname = f\"{name.replace(' ', '_')}_tone\"\n",
    "    tone_df = (filtered_df.groupby(F.date_format(\"DATE\", \"y-MM-dd\").alias(\"date\"))\n",
    "                          .agg((F.sum(\"Tone\") / F.sum(\"WordCount\")).alias(colname))\n",
    "                          .select(\"date\", f\"{colname}\")\n",
    "                          .withColumn(\"date\", F.to_date(\"date\", format=\"y-MM-dd\"))\n",
    "                          .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
    "                          .orderBy(F.col(\"date\").asc())\n",
    "              )\n",
    "    return tone_df\n",
    "\n",
    "def subtract_cols(df, col1, col2):\n",
    "    df = (df.withColumn(col1, df[f\"{col1}\"] - df[f\"{col2}\"])\n",
    "            .withColumnRenamed(col1, col1.replace(\"_tone\", \"_diff\")))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_col_avgs(df):\n",
    "    exclude = [k for k, v in df.dtypes if v in [\"date\", \"timestamp\", \"string\", \"SourceCommonName\"]]\n",
    "    avgs = df.select([F.avg(c).alias(c) for c in df.columns if c not in exclude]).collect()[0]\n",
    "    return {c: avgs[c] for c in df.columns if c not in exclude}\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def show_df_rounded(df, places=4, rows=20):\n",
    "    dtypes = {k: v for k, v in df.dtypes}\n",
    "    date_cols = [k for k, v in dtypes.items() if v in [\"date\", \"timestamp\"]]\n",
    "    str_cols = [k for k, v in dtypes.items() if v == \"string\"]\n",
    "    int_cols = [k for k, v in dtypes.items() if \"int\" in v]\n",
    "    \n",
    "    show_cols = [F.date_format(c, \"y-MM-dd\").alias(c) if c in date_cols\n",
    "                 else (F.col(c).alias(c) if c in str_cols\n",
    "                 else (F.format_number(c, 0).alias(c) if c in int_cols\n",
    "                 else (F.format_number(c, places).alias(c))))\n",
    "                 for c in df.columns]\n",
    "    show_cols = [c for c in show_cols]\n",
    "    df.select(*show_cols).limit(rows).show()\n",
    "\n",
    "# DBTITLE 1,Load Data from Delta Table\n",
    "def load_data(save_path, file_name): \n",
    "  df = (spark.read.format(\"delta\")\n",
    "                      .option(\"header\", \"true\")\n",
    "                      .option(\"inferSchema\", \"true\")\n",
    "                      .load(os.path.join(save_path, file_name))\n",
    "           )\n",
    "  return df.toPandas()\n",
    "\n",
    "\n",
    "def filter_non_esg(df): \n",
    "    return df[(df['E']==True) | (df['S'] == True) | (df['G'] == True)]\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "class graph_creator:\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def create_graph(self):\n",
    "        # Find Edges\n",
    "        df_edge = pd.DataFrame(self.df.groupby(\"URL\").Organization.apply(list)\n",
    "                               ).reset_index()\n",
    "\n",
    "        get_tpls = lambda r: (list(itertools.combinations(r, 2)) if\n",
    "                              len(r) > 1 else None)\n",
    "        df_edge[\"SourceDest\"] = df_edge.Organization.apply(get_tpls)\n",
    "        df_edge = df_edge.explode(\"SourceDest\").dropna(subset=[\"SourceDest\"])\n",
    "\n",
    "        # Get Weights\n",
    "        source_dest = pd.DataFrame(df_edge.SourceDest.tolist(),\n",
    "                                   columns=[\"Source\", \"Dest\"])\n",
    "        sd_mapping = source_dest.groupby([\"Source\", \"Dest\"]).size()\n",
    "        get_weight = lambda r: sd_mapping[r.Source, r.Dest]\n",
    "        source_dest[\"weight\"] = source_dest.apply(get_weight, axis=1)\n",
    "\n",
    "        # Get\n",
    "        self.organizations = set(source_dest.Source.unique()).union(\n",
    "                             set(source_dest.Dest.unique()))\n",
    "        self.G = nx.from_pandas_edgelist(source_dest, source=\"Source\",\n",
    "            target=\"Dest\", edge_attr=\"weight\", create_using=nx.Graph)\n",
    "        return self.G\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_embeddings(G, organizations):\n",
    "    # Fit graph\n",
    "    g2v = NVVV()\n",
    "    g2v.fit(G)\n",
    "    \n",
    "    # Embeddings\n",
    "    embeddings = g2v.model.wv.vectors\n",
    "    pca = PCA(n_components=3)\n",
    "    principalComponents = pca.fit_transform(embeddings)\n",
    "    d_e = pd.DataFrame(principalComponents)\n",
    "    d_e[\"company\"] = organizations\n",
    "    return d_e, g2v\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def get_connections(organizations, topn=25):\n",
    "    l = [g2v.model.wv.most_similar(org, topn=topn)\n",
    "         for org in organizations]\n",
    "    df_sim = pd.DataFrame(l, columns=[f\"n{i}\" for i in range(topn)])\n",
    "    for col in df_sim.columns:\n",
    "        new_cols = [f\"{col}_rec\", f\"{col}_conf\"]\n",
    "        df_sim[new_cols] = pd.DataFrame(df_sim[col].tolist(), \n",
    "                                        index=df_sim.index)\n",
    "    df_sim = df_sim.drop(columns=[f\"n{i}\" for i in range(topn)])\n",
    "    df_sim.insert(0, \"company\", list(organizations))\n",
    "    return df_sim\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def make_embeddings_and_connections(start, end):\n",
    "    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_Russell_top_300\"\n",
    "    save_dir = os.path.join(base_dir, f\"{start}__to__{end}\")\n",
    "    csv_file = \"data_as_csv.csv\"\n",
    "\n",
    "    # Load data\n",
    "    print(\"Loading Data\")\n",
    "    df = pd.read_csv(os.path.join(save_dir, csv_file).replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "    df = filter_non_esg(df)\n",
    "\n",
    "    # Create graph\n",
    "    print(\"Creating Graph\")\n",
    "    creator = graph_creator(df)\n",
    "    G = creator.create_graph()\n",
    "    organizations = list(creator.organizations)\n",
    "\n",
    "    # Save graph as pkl\n",
    "    fp = os.path.join(save_dir, \"organization_graph.pkl\").replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    with open(fp, \"wb\") as f:\n",
    "        pickle.dump(G, f)\n",
    "        \n",
    "    # Create embeddings\n",
    "    print(\"Creating embeddings\")\n",
    "    emb_path = os.path.join(save_dir, \"pca_embeddings.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    d_e, g2v = get_embeddings(G, organizations)\n",
    "    d_e.to_csv(emb_path, index=False)\n",
    "    \n",
    "    # Create connections\n",
    "    print(\"Creating connections\")\n",
    "    df_sim = get_connections(organizations)\n",
    "    sim_path = os.path.join(save_dir, \"connections.csv\")\n",
    "    df_sim.to_csv(sim_path.replace(\"dbfs:/\", \"/dbfs/\"))\n",
    "    \n",
    "    # Save organizations as delta\n",
    "    conn_path = os.path.join(save_dir, \"CONNECTIONS\")\n",
    "    conn_data = spark.createDataFrame(df_sim)\n",
    "    conn_data.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(conn_path)\n",
    "    \n",
    "def make_tables(start_date, end_date):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Directories\n",
    "    org_types = f\"Russell_top_{Fields.n_orgs}\"\n",
    "    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_{org_types}\"\n",
    "    range_save_dir = os.path.join(base_dir, f\"{start_date}__to__{end_date}\")\n",
    "    esg_dir = os.path.join(range_save_dir, \"esg_scores\")\n",
    "    dbutils.fs.mkdirs(esg_dir)\n",
    "    \n",
    "    # Load data\n",
    "    data_path = os.path.join(range_save_dir, \"data_as_delta\")\n",
    "    try:\n",
    "        data = (spark.read.format(\"delta\")\n",
    "                     .option(\"header\", \"true\")\n",
    "                     .option(\"inferSchema\", \"true\")\n",
    "                     .load(data_path)\n",
    "               )\n",
    "        print(\"Data Loaded!\")\n",
    "    except:\n",
    "        print(\"Data for these dates hasn't been generated!!!\")\n",
    "        return\n",
    "\n",
    "    # Get all organizations\n",
    "    print(\"Finding all Organizations\")\n",
    "    organizations = [x.Organization for x in data.select(\n",
    "                     \"Organization\").distinct().collect()]\n",
    "    \n",
    "    # Get the overall tone\n",
    "    print(\"Calculating Tones Over Time\")\n",
    "    overall_tone = daily_tone(data, \"industry\")\n",
    "    esg_tones = {L: daily_tone(data.filter(f\"{L} == True\"), \"industry\")\n",
    "                 for L in [\"E\", \"S\", \"G\"]}\n",
    "    \n",
    "    # Loop through the organizations to get the average daily tone for each company\n",
    "    pct_idxs = range(0, len(organizations), len(organizations) // 10)\n",
    "    for i, org in enumerate(organizations):\n",
    "        if i in pct_idxs:\n",
    "            print(f\"{pct_idxs.index(i) * 10}%\")\n",
    "        tone_label = f\"{org.replace(' ', '_')}_tone\"\n",
    "        \n",
    "        overall_org_df = data.filter(f\"Organization == '{org}'\")\n",
    "        org_tone = daily_tone(overall_org_df, org)\n",
    "        overall_tone = subtract_cols(overall_tone.join(org_tone, on=\"date\", how=\"left\"),\n",
    "                                     tone_label, \"industry_tone\")\n",
    "      \n",
    "        for L, tdf in esg_tones.items():\n",
    "            esg_org_df = overall_org_df.filter(f\"{L} == True\")\n",
    "            esg_org_tone = daily_tone(esg_org_df, org)\n",
    "            esg_tones[L] = subtract_cols(tdf.join(esg_org_tone, on=\"date\", how=\"left\"), \n",
    "                                         tone_label, \"industry_tone\")            \n",
    "    del data   \n",
    "    \n",
    "    # Average to get overall scores\n",
    "    print(\"Computing Overall Scores\")\n",
    "    scores = {}\n",
    "    overall_scores = get_col_avgs(overall_tone)\n",
    "    esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n",
    "\n",
    "    for org in organizations:\n",
    "        diff_label = f\"{org.replace(' ', '_')}_diff\"\n",
    "        scores[org] = {L: tdf[diff_label] for L, tdf in esg_scores.items()}\n",
    "        scores[org][\"T\"] = overall_scores[diff_label]\n",
    "      \n",
    "      \n",
    "    # Save all the tables\n",
    "    print(\"Saving Tables\")  \n",
    "    \n",
    "    # Overall ESG\n",
    "    print(\"    Daily Overall ESG\")\n",
    "    path = os.path.join(esg_dir, \"overall_daily_esg_scores.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    pd_df = overall_tone.toPandas().set_index(\"date\").sort_index().asfreq(freq=\"D\", method=\"ffill\")\n",
    "    pd_df.to_csv(path, index=True)\n",
    "    \n",
    "    # E, S, and G\n",
    "    for L, tdf in esg_tones.items():\n",
    "        print(\"    Daily \" + L)\n",
    "        path = os.path.join(esg_dir, f\"daily_{L}_score.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n",
    "        pd_df = tdf.toPandas().set_index(\"date\").sort_index().asfreq(freq=\"D\", method=\"ffill\")\n",
    "        pd_df.to_csv(path, index=True)\n",
    "\n",
    "    # Averaged scores\n",
    "    print(\"    Average Scores\")\n",
    "    score_path = path = os.path.join(esg_dir, \"average_esg_scores.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n",
    "    pd.DataFrame(scores).to_csv(score_path, index=True)\n",
    "    print(\"DONE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_esg_values(start_date, end_date):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"calulation started\")\n",
    "    data_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\"\n",
    "    try:\n",
    "        data = (spark.read.format(\"delta\")\n",
    "                     .option(\"header\", \"true\")\n",
    "                     .option(\"inferSchema\", \"true\")\n",
    "                     .load(data_path)\n",
    "               )\n",
    "        print(\"Data Loaded!\")\n",
    "    except:\n",
    "        print(\"Data for these dates hasn't been generated!!!\")\n",
    "        return\n",
    "\n",
    "    # Get all organizations\n",
    "    print(\"Finding all Organizations\")\n",
    "    organizations = [x.Organization for x in data.select(\n",
    "                     \"Organization\").distinct().collect()]\n",
    "    \n",
    "    # Get the overall tone\n",
    "    print(organizations)\n",
    "    print(\"Calculating Tones Over Time\")\n",
    "    overall_tone = avg_day_tone(singleSource(data), \"industry\")\n",
    "    #overall_tone = avg_day_tone(data, \"industry\")\n",
    "    esg_tones = {L: avg_day_tone(singleSource(data.filter(f\"{L} == True\")), \"industry\")\n",
    "                 for L in [\"E\", \"S\", \"G\"]}\n",
    "    \n",
    "    pct_idxs = range(0, len(organizations))\n",
    "    \n",
    "    for i, org in enumerate(organizations):\n",
    "        tone_label = f\"{org.replace(' ', '_')}_tone\"\n",
    "\n",
    "        overall_org_df = data.filter(f\"Organization == '{org}'\")\n",
    "        org_tone = avg_day_tone(singleSource(overall_org_df), org)\n",
    "\n",
    "        overall_tone = overall_tone.join(org_tone, on=\"date\", how=\"left\")\n",
    "      \n",
    "        for L, tdf in esg_tones.items():\n",
    "            esg_org_df = overall_org_df.filter(f\"{L} == True\")\n",
    "            esg_org_tone = avg_day_tone(singleSource(esg_org_df), org)\n",
    "\n",
    "            \n",
    "            esg_tones[L] = tdf.join(esg_org_tone, on=\"date\", how=\"left\")\n",
    "    \n",
    "    \n",
    "         \n",
    "    del data   \n",
    "    \n",
    "    \n",
    "    # Average to get overall scores\n",
    "    print(organizations)\n",
    "    print(\"Computing Overall Scores\")\n",
    "    scores = {}\n",
    "    print(\"    Calculating overall tone\")\n",
    "    overall_scores = get_col_avgs(overall_tone)\n",
    "    print(\"    Calculating esg tone\")\n",
    "    esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n",
    "    \n",
    "    #print(overall_scores)\n",
    "    print(esg_scores)\n",
    "    print(\"DONE!\")\n",
    "    return esg_scores, overall_tone\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06496dbe-0dad-4ceb-b16c-8340047e555a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_company_data(base_dir, start_date, end_date, org_name):\n",
    "    #print(org_name)\n",
    "    _ = download_gdelt_data(start_date, end_date,org_name, save_csv=True)\n",
    "    #print(org_name)\n",
    "    esg_score1, _ = calculating_esg_values(start_date, end_date)\n",
    "    return esg_score1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating ESG Scores for input Companies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"dbfs:/mnt/esg/financial_report_data\" \n",
    "dbutils.widgets.text(\"myinput\",\"microsoft;apple\")\n",
    "dbutils.widgets.text(\"startdate\",\"2022-05-01\")\n",
    "dbutils.widgets.text(\"enddate\",\"2022-05-02\")\n",
    "\n",
    "var_a = dbutils.widgets.get(\"myinput\")\n",
    "var_a = var_a.split(';')\n",
    "print(var_a)\n",
    "start_date = dbutils.widgets.get(\"startdate\")\n",
    "end_date = dbutils.widgets.get(\"enddate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61e8c5fc-2dfe-4918-8917-34243d699e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;microsoft&#39;]\n",
       "\n",
       "Loading and cleaning all data\n",
       "20220101 0000\n",
       "I am here\n",
       "(1085, 27)\n",
       "(1085, 14)\n",
       "done c=0\n",
       "20220101 0015\n",
       "I am here\n",
       "(874, 27)\n",
       "(874, 14)\n",
       "appending\n",
       "20220101 0030\n",
       "I am here\n",
       "(964, 27)\n",
       "(964, 14)\n",
       "appending\n",
       "20220101 0045\n",
       "I am here\n",
       "(1014, 27)\n",
       "(1014, 14)\n",
       "appending\n",
       "20220101 0100\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[&#39;microsoft&#39;]\n\nLoading and cleaning all data\n20220101 0000\nI am here\n(1085, 27)\n(1085, 14)\ndone c=0\n20220101 0015\nI am here\n(874, 27)\n(874, 14)\nappending\n20220101 0030\nI am here\n(964, 27)\n(964, 14)\nappending\n20220101 0045\nI am here\n(1014, 27)\n(1014, 14)\nappending\n20220101 0100\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = download_company_data(base_dir, start_date, end_date, var_a)\n",
    "dbutils.notebook.exit(output)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "esg_final_one1 (3)",
   "notebookOrigID": 1451327037292386,
   "widgets": {
    "myinput": {
     "currentValue": "microsoft",
     "nuid": "d665e895-3192-4cd3-bdca-caf8ac4bc19b",
     "widgetInfo": {
      "defaultValue": "microsoft",
      "label": null,
      "name": "myinput",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
