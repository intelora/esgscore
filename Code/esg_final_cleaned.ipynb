{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b123039e-ef0b-47a3-a385-36ae3147d067","showTitle":false,"title":""}},"source":["## Import Required Packages:"]},{"cell_type":"code","execution_count":1,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d97036f9-4f1d-41a8-a5b2-373977907936","showTitle":false,"title":""}},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'pyspark'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[1;32mc:\\Users\\MEGH\\Desktop\\GamePortal-code\\esg_final_cleaned.ipynb Cell 2\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MEGH/Desktop/GamePortal-code/esg_final_cleaned.ipynb#W1sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpprint\u001b[39;00m \u001b[39mimport\u001b[39;00m pprint\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MEGH/Desktop/GamePortal-code/esg_final_cleaned.ipynb#W1sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/MEGH/Desktop/GamePortal-code/esg_final_cleaned.ipynb#W1sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mfunctions\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mF\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MEGH/Desktop/GamePortal-code/esg_final_cleaned.ipynb#W1sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m \u001b[39mimport\u001b[39;00m DataFrame \u001b[39mas\u001b[39;00m spark_DataFrame\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/MEGH/Desktop/GamePortal-code/esg_final_cleaned.ipynb#W1sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n","\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"]}],"source":["import os\n","import datetime\n","from collections import Counter\n","from pprint import pprint\n","import pandas as pd\n","import pyspark.sql.functions as F\n","from pyspark.sql import DataFrame as spark_DataFrame\n","from pyspark.sql.types import *\n","# from functools import reduce\n","import matplotlib.pyplot as plt\n","import requests\n","import re\n","from itertools import product\n","import time\n","from io import BytesIO\n","import requests\n","import numpy as np\n","from pyspark.sql import Window\n","import sys\n","#---------------------------- additional level2 packages ---------------------------------\n","from pyspark.sql.column import Column # _to_seq\n","from pyspark import copy_func, since\n","from pyspark.context import SparkContext\n","import genericpath\n","from genericpath import *\n","import stat\n","from pyspark.sql.column import _to_seq\n","import builtins\n","#------------------------------------- base import depencency ---------------------------\n","from __future__ import annotations\n","from pandas._libs import (NaT,Period,Timestamp,index as libindex,lib,)\n","from pandas.util._decorators import (\n","    Appender,\n","    Substitution,\n","    deprecate_kwarg,\n","    deprecate_nonkeyword_arguments,\n","    doc,\n","    rewrite_axis_style_signature,\n",")\n","from pandas._libs.lib import no_default"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5d411e61-d25c-47f2-8908-7e3e7548c725","showTitle":false,"title":""}},"source":["### Utility Functions"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"40ab4205-e2ba-4b7e-afab-37e6a7431a53","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# base level data point to start \n","_iord_zero_point = object()\n","# core engine algorithm for calculation and data arrangement \n","def iord_algo(function, sequence, initial=_iord_zero_point):          \n","    \"\"\"\n","    iord_algo(function, sequence[, initial]) -> value\n","\n","    This is function of two arguments cumulatively of items of sequence, in right hand direction,\n","    so as to decrease sequence to single value. For example, iord_algo(lambda x, y: x+y, [1, 2, 3, 4, 5]) \n","    process ((((1+2)+3)+4)+5).  Incase initial is avilable, it will be placed before the item\n","    of the sequence in the process, and serves as a default when for empty sequence.\n","    \"\"\"\n","\n","    iord_point = iter(sequence)\n","\n","    if initial is _iord_zero_point:\n","        try:\n","            evaluate = next(iord_point)\n","        except StopIteration:\n","            raise TypeError(\"iord_algo() of empty sequence with no initial value\") from None\n","    else:\n","        evaluate = initial\n","        \n","    for element in iord_point:\n","        evaluate = function(evaluate, element)\n","\n","    return evaluate"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"ffc7bc18-090d-448d-a157-8151309cfa88","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["#to give a lable to column\n","def _apply_grid_label_name(name):\n","    sc = SparkContext._active_spark_context\n","    return sc._jvm.functions.col(name)\n","\n","if sys.version >= '3':\n","    basestring = str\n","    long = int\n","\n","#to apply label with java jvm call\n","def _jvm_grid_label(col):\n","    if isinstance(col, Column):\n","        jcol = col._jc\n","    elif isinstance(col, basestring):\n","        jcol = _apply_grid_label_name(col)\n","    else:\n","        raise TypeError(\n","            \"Invalid argument, should be either string or column: \"\n","            \"{0} of type {1}. \"\n","            \"For cols literals, please use lit, array, struct or create_map \"\n","            \"functions.\".format(col, type(col)))\n","    return jcol\n","\n","# convert date format into proper alignment\n","def date_alignment(date, format):\n","    sc = SparkContext._active_spark_context\n","    return Column(sc._jvm.functions.date_format(_jvm_grid_label(date), format))\n","\n","# handle and validate begning date to pass jvm \n","def beginning_date(col, format=None):\n","    sc = SparkContext._active_spark_context\n","    if format is None:\n","        jc = sc._jvm.functions.to_date(_jvm_grid_label(col))\n","    else:\n","        jc = sc._jvm.functions.to_date(_jvm_grid_label(col), format)\n","    return Column(jc)\n","\n","# to re arrange columns for require format \n","def intelora_rearrange_handler(col):\n","    sc = SparkContext._active_spark_context\n","    jc = sc._jvm.functions.explode(_jvm_grid_label(col))\n","    return Column(jc)"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f9423e24-f7cf-4a93-b777-f86236a7041c","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# align data with column and grid scale\n","def intelora_data_aligner(                                      # line no. 717 def create_graph in explode\n","        self,\n","        column: IndexLabel,\n","        ignore_index: bool = False,\n","    ) -> DataFrame:\n","        if not self.columns.is_unique:\n","            raise ValueError(\"columns is not unique, it has to be be unique-001\")\n","\n","        columns: list[Hashable]\n","        if is_scalar(column) or isinstance(column, tuple):\n","            columns = [column]\n","        elif isinstance(column, list) and all(\n","            map(lambda c: is_scalar(c) or isinstance(c, tuple), column)\n","        ):\n","            if not column:\n","                raise ValueError(\"column is empty, it should be nonempty\")\n","            if len(column) > len(set(column)):\n","                raise ValueError(\"columns is not unique, it has to be be unique-002\")\n","            columns = column\n","        else:\n","            raise ValueError(\"column is nut amoung any of scalar, tuple, or list thereof \")\n","\n","        df = self.reset_index(drop=True)\n","        if len(columns) == 1:\n","            result = df[columns[0]].explode()\n","        else:\n","            mylen = lambda x: len(x) if is_list_like(x) else -1\n","            counts0 = self[columns[0]].apply(mylen)\n","            for c in columns[1:]:\n","                if not all(counts0 == self[c].apply(mylen)):\n","                    raise ValueError(\"columns should match element counts members\")\n","            result = DataFrame({c: df[c].explode() for c in columns})\n","        result = df.drop(columns, axis=1).join(result)\n","        if ignore_index:\n","            result.index = default_index(len(result))\n","        else:\n","            result.index = self.index.take(result.index)\n","        result = result.reindex(columns=self.columns, copy=False)\n","        return result\n","\n","# ------------------------ data structure for matching purpose -----------------------------------\n","def intelora_dataset_matcher(                                               # line no.  def apply() \n","    obj: DataFrame,\n","    func: AggFuncType,\n","    axis: Axis = 0,\n","    raw: bool = False,\n","    result_type: str | None = None,\n","    args=None,\n","    kwargs=None,\n",") -> FrameApply:\n","    \"\"\"It build and return a row or column based frame required object\"\"\"\n","    axis = obj._get_axis_number(axis)\n","    klass: type[FrameApply]\n","    if axis == 0:\n","        klass = FrameRowApply\n","    elif axis == 1:\n","        klass = FrameColumnApply\n","\n","    return klass(\n","        obj,\n","        func,\n","        raw=raw,\n","        result_type=result_type,\n","        args=args,\n","        kwargs=kwargs,\n","    )\n","# ------------------------------------- clubbing data operation --------------------------------------------\n","def intelora_add(                                             #  line no. 722 in create grafh\n","        self,\n","        func: AggFuncType,\n","        axis: Axis = 0,\n","        raw: bool = False,\n","        result_type=None,\n","        args=(),\n","        **kwargs,\n","    ):\n","        # the core appy of data frame matcher\n","        op = intelora_dataset_matcher(\n","            self,\n","            func=func,\n","            axis=axis,\n","            raw=raw,\n","            result_type=result_type,\n","            args=args,\n","            kwargs=kwargs,\n","        )\n","        return op.apply().__finalize__(self, method=\"apply\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4e32434e-e0c2-46ba-8e69-66d963dac161","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["# filter out data and argument with none and index label\n","@deprecate_nonkeyword_arguments(version=None, allowed_args=[\"self\"])\n","def to_clean_require(\n","    self,\n","    axis: Axis = 0,\n","    how: str = \"any\",\n","    thresh=None,\n","    subset: IndexLabel = None,\n","    inplace: bool = False,\n","    ):\n","        inplace = validate_bool_kwarg(inplace, \"inplace\")\n","        if isinstance(axis, (tuple, list)):\n","            raise TypeError(\" it not support anymore supplying multiple axes to axis.\")\n","        axis = self._get_axis_number(axis)\n","        agg_axis = 1 - axis\n","\n","        agg_obj = self\n","        if subset is not None:\n","            # subset require list format only\n","            if not is_list_like(subset):\n","                subset = [subset]\n","            ax = self._get_axis(agg_axis)\n","            indices = ax.get_indexer_for(subset)\n","            check = indices == -1\n","            if check.any():\n","                raise KeyError(np.array(subset)[check].tolist())\n","            agg_obj = self.take(indices, axis=agg_axis)\n","\n","        if thresh is not None:\n","            count = agg_obj.count(axis=agg_axis)\n","            mask = count >= thresh\n","        elif how == \"any\":\n","            # faster compare to 'agg_obj.count(agg_axis) == self.shape[agg_axis]'\n","            mask = notna(agg_obj).all(axis=agg_axis, bool_only=False)\n","        elif how == \"all\":\n","            # faster compare to 'agg_obj.count(agg_axis) > 0'\n","            mask = notna(agg_obj).any(axis=agg_axis, bool_only=False)\n","        else:\n","            if how is not None:\n","                raise ValueError(f\"not valid how option: {how}\")\n","            else:\n","                raise TypeError(\"should specify how or thresh\")\n","\n","        if np.all(mask):\n","            result = self.copy()\n","        else:\n","            result = self.loc(axis=axis)[mask]\n","\n","        if inplace:\n","            self._update_inplace(result)\n","        else:\n","            return result\n","\n","#------------------------------------- data structure collpse ------------------------------------------------------\n","def intelora_arranger(              # sd_mapping def create_graph \n","    self,\n","    by=None,\n","    axis: Axis = 0,\n","    level: Level | None = None,\n","    as_index: bool = True,\n","    sort: bool = True,\n","    group_keys: bool = True,\n","    squeeze: bool | lib.NoDefault = no_default,\n","    observed: bool = False,\n","    dropna: bool = True,\n",") -> DataFrameGroupBy:\n","    from pandas.core.groupby.generic import DataFrameGroupBy\n","    # validating data in sequence\n","    if squeeze is not no_default:\n","        warnings.warn(\n","            (\n","               \" squeeze parameter is no longer supported and \"\n","                \"suppose to removed later.\"\n","            ),\n","            FutureWarning,\n","            stacklevel=find_stack_level(),\n","        )\n","    else:\n","        squeeze = False\n","\n","    if level is None and by is None:\n","        raise TypeError(\"You have to supply one of 'by' and 'level'\")\n","    axis = self._get_axis_number(axis)\n","    return DataFrameGroupBy(\n","        obj=self,\n","        keys=by,\n","        axis=axis,\n","        level=level,\n","        as_index=as_index,\n","        sort=sort,\n","        group_keys=group_keys,\n","        squeeze=squeeze,  # type: ignore[arg-type]\n","        observed=observed,\n","        dropna=dropna,\n","    )"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aedc92c1-8c20-406f-b993-2ba95ecabd4c","showTitle":false,"title":""}},"outputs":[],"source":["# apply call leavel data predefined rule \n","def also_call_as(c):\n","    \"\"\"\n","    create list of aliases for given company using predefined rules.\n","    Try to encompass as many options as possible keeping so that user can customize,\n","    containg aliases may left out.\n","    \"\"\"\n","    c = c.lower()\n","\n","    # maintain URLs string such as .com\n","    if len(c) > 3 and c[-4] == \".\":\n","        a = c\n","        c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n","        aliases = set([c, a])\n","    else:\n","        aliases = set([c])\n","\n","    # get last character of endings\n","    if len(c.split()[-1]) == 1:\n","        c = c.rsplit(\" \", 1)[0]\n","        aliases.add(c)\n","\n","    # Company legal endings\n","    endings = [\"inc\",\n","               \"corp\",\n","               \"plc\", \n","               \"reit\", \n","               \"co\", \n","               \"cor\", \n","               \"group\", \n","               \"company\",\n","               \"trust\",\n","               \"energy\",\n","               \"international\",\n","               \"of america\",\n","               \"pharmaceuticals\",\n","               \"clas\",\n","               \"in\", \"nv\",\n","               \"sa\", \n","               \"re\", \n","               \"pvt ltd\",\n","               \"private limited\" ,\n","               \"india private limited\"\n","               \"Co\",\n","               \"CO.\",\n","               \"Companies\",\n","               \"Company\",\n","               \"Corp\",\n","               \"CORP.\",\n","               \"Corporation\",\n","               \"Inc\",\n","               \"INC.\",\n","               \"Incorporated\",\n","               \"Limited\",\n","               \"Ltd\",\n","               \"Professional Corporation\",\n","               \"Chartered\",\n","               \"Limited\",\n","               \"Ltd\",\n","               \"Ltd.\",\n","               \"pa\",\n","               \"p.c \",\n","               \"Professional Association\",\n","               \"Professional Corporation\",\n","               \"Professional Service Corporation\",\n","               \"psc\",\n","               \"sc.\",\n","               \"Service Corporation\"]\n","    \n","    n_endings = 3  # Can have up to 3 of these endings\n","    for _ in range(n_endings):\n","        aliases.update([a.rsplit(\" \", 1)[0] for a in aliases if\n","                        any([a.endswith(\" \" + e) for e in endings])])\n","        c = c.rsplit(\" \", 1)[0] if any([c.endswith(\" \" + e) for e in endings]) else c\n","\n","    # Alias any dashes and replace in company name\n","    aliases.update([a.replace(\"-\", \"\") for a in aliases] +\n","                   [a.replace(\"-\", \" \") for a in aliases])\n","    c = c.replace(\"-\", \" \")\n","\n","    # If '&' stands on its own, add alias of 'and'\n","    aliases.update([a.replace(\" & \", \" and \") for a in aliases])\n","    return {c: list(aliases)}\n","\n","# get column alias from compnay \n","def generate_common_org_names(companies):\n","    \"\"\"\n","    store data of  companies and loop through them to find aliases\n","    \"\"\"\n","    #companies = [org_name]\n","    comp_dict = dict()\n","    for c in companies:\n","        comp_dict.update(also_call_as(c))\n","    return comp_dict\n","\n","#using spark dataframe union for list\n","combine_spark_dfs = lambda sdf_list: iord_algo(spark_DataFrame.union, sdf_list)\n","\n","# a global level common data defination\n","class Common_MetaData:\n","    \"\"\" Variables to use across many functions. \"\"\"\n","    keep = [\"DATE\",\n","            \"SourceCommonName\", \n","            \"DocumentIdentifier\", \n","            \"Themes\",\n","            \"Organizations\",\n","            \"V2Tone\"]\n","    tone = [\"Tone\",\n","            \"PositiveTone\",\n","            \"NegativeTone\", \n","            \"Polarity\",\n","            \"ActivityDensity\", \n","            \"SelfDensity\",\n","            \"WordCount\"]\n","    organizations = None\n","\n","# spark based expand basic dataset\n","@udf(ArrayType(StringType(), True))\n","def simple_expand_spark(x):\n","    \"\"\" Expand semicolon token strint to a list (also avoid empty)\"\"\"\n","    if not x:\n","        return []\n","    return list(filter(None, x.split(\";\")))\n","\n","# spark based tone data generation\n","@udf(MapType(StringType(), DoubleType()))\n","def tone_expand_spark(x):\n","    \"\"\" Expand and identify tone field. \"\"\"\n","    if not x:\n","        return {t: None for t in Common_MetaData.tone}\n","    return {Common_MetaData.tone[i]: float(v) for i, v in enumerate(x.split(\",\"))}\n","\n","# spark template validation for stream\n","@udf(BooleanType())\n","def has_theme_spark(x, theme):\n","    \"\"\" Is the given theme included in any of the listed themes? \"\"\"\n","    return any([theme in lst.split(\"_\") for lst in x])\n","\n","# cleanup process for org data \n","@udf(StringType())\n","def clean_organization(s):\n","    \"\"\" Standardize the organization names. \"\"\"\n","    for k, v in Common_MetaData.organizations.items():\n","        if v[0] in s.split() :\n","            return k\n","    return s.lower()\n","\n","## alias of company and compass data value\n","# def also_call_create(c):\n","#     \"\"\"\n","#     create list of aliases for require company using few predefined rules.\n","#     Try to encompass many options with possible keeping possibality,\n","#     keep needed aliases left out.\n","#     \"\"\"\n","#     c = c.lower()\n","\n","#     # URLs (e.g. \".com\")\n","#     if len(c) > 3 and c[-4] == \".\":\n","#         a = c\n","#         c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n","#         aliases = set([c, a])\n","#     else:\n","#         aliases = set([c])\n","#     return aliases"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9e0a731b-1a7d-49a2-b339-26b9b6f32a90","showTitle":false,"title":""}},"source":["## Downloading , Filtering and Storing Gdelt Data:"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3ed691b2-9044-4bd4-ba48-3d72ef2052d4","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["class Redesign_data_format:\n","    def redesign_sdf(self, sdf, file_path_refined, org_name):\n","        \"\"\"\n","        Given a spark data frame of the downloaded data, reformat it\n","        into human-readable Common_MetaData.\n","        Add a few more Common_MetaData for our purposes.\n","        \"\"\"\n","        setattr(Common_MetaData, \"organizations\",generate_common_org_names(org_name))\n","        \n","        sdf = sdf.select(*Common_MetaData.keep)\n","        \n","        if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n","            # Reformat existing columns\n","            sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n","                      .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n","                      .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n","                      .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n","                   )\n","\n","            # Create ESG columns & explode organization column\n","            sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n","                      .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n","                      .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n","                   )\n","        \n","            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n","            print(\"sdf created\")\n","        \n","        else:\n","            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n","        \n","        sdf = (sdf.withColumn(\"Organization\", intelora_rearrange_handler(\"Organizations\"))\n","                  .withColumn(\"Organization\", clean_organization(\"organization\"))\n","                  .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n","              )\n","\n","        # Expand tone columns\n","        exprs = [F.col(\"V2Tone\").getItem(k).alias(k) for k in Common_MetaData.tone]\n","\n","        sdf = sdf.select(*sdf.columns, *exprs).drop(\"V2Tone\")\n","        #print(sdf.column)\n","        return sdf\n","\n","\n","#     def download_and_generate_gdelt_table1(self, date, gd):\n","#         \"\"\"\n","#         Download the GDELT table as a pandas dataframe using the gdelt package.\n","#         Return a spark data frame.\n","#         \"\"\"\n","#         pdf = gd.Search([date], table=\"gkg\",coverage=True, output=\"df\")\n","#         pdf[\"DATE\"] = pd.to_datetime(pdf[\"DATE\"], format=\"%Y%m%d%H%M%S\")\n","\n","#         sdf = spark.createDataFrame(pdf)\n","#         print(\"   * loaded *  \", date)\n","#         return sdf\n","    \n","    def download_and_generate_gdelt_table(self, date, file_path):\n","        file_path_os =  file_path.replace(\"dbfs:/\", \"/dbfs/\")\n","        s = []\n","        for i in range(24):\n","            for j in list(range(0, 60, 15)):\n","                if i<10:\n","                    if j<10:\n","                        s.append('0' + str(i) + '0' + str(j))\n","                    else:\n","                        s.append('0' + str(i) + str(j))\n","                else:\n","                    if j<10:\n","                        s.append(str(i) + '0' + str(j))\n","                    else:\n","                         s.append(str(i) + str(j))\n","                            \n","        if not os.path.exists(file_path_os):\n","            li = ''.join(date.split('-'))\n","            c=0\n","            df1 = None\n","            for elem in s:\n","                try:\n","                    print(li, elem)\n","                    response = requests.get('http://data.gdeltproject.org/gdeltv2/'+li+ elem + '00.gkg.csv.zip')\n","                    #'http://data.gdeltproject.org/gkg/20220204.gkg.csv.zip')\n","                    buffer = BytesIO(response.content)\n","                   \n","                    frame = pd.read_csv(buffer, compression='zip', sep='\\t',header=None, warn_bad_lines=True,encoding='latin')\n","                    frame[1] = pd.to_datetime(frame[1], format=\"%Y%m%d%H%M%S\")\n","        \n","                    frame.columns = ['GKGRECORDID', \n","                                     'DATE', \n","                                     'SourceCollectionIdentifier', \n","                                     'SourceCommonName',\n","                                     'DocumentIdentifier',\n","                                     'Counts',\n","                                     'V2Counts', \n","                                     'Themes', \n","                                     'V2Themes',\n","                                     'Locations',\n","                                     'V2Locations', \n","                                     'Persons', \n","                                     'V2Persons', \n","                                     'Organizations',\n","                                     'V2Organizations', \n","                                     'V2Tone',\n","                                     'Dates',\n","                                     'GCAM',\n","                                     'SharingImage',\n","                                     'RelatedImages', \n","                                     'SocialImageEmbeds', \n","                                     'SocialVideoEmbeds', \n","                                     'Quotations',\n","                                     'AllNames', \n","                                     'Amounts', \n","                                     'TranslationInfo', \n","                                     'Extras'] \n","                    columns1 = ['DATE', \n","                                'SourceCollectionIdentifier', \n","                                'SourceCommonName',\n","                                'DocumentIdentifier',\n","                                'Counts', \n","                                'V2Counts',\n","                                'Themes',\n","                                'V2Themes',\n","                                'Locations',\n","                                'V2Locations',\n","                                'Organizations',\n","                                'V2Organizations',\n","                                'V2Tone',\n","                                'Dates'] \n","                    frame = frame[columns1]\n","                    print(frame.shape)\n","                    if c==0:\n","                        df1 = frame\n","                        c=1\n","                    else:\n","                        df1 = df1.append(frame, ignore_index=True)\n","                except:\n","                    pass\n","            \n","            sdf = spark.createDataFrame(df1)\n","            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path)\n","            print(\"   * loaded *  \", date)\n","        else:\n","            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path)\n","        return sdf\n","\n","\n","    def getting_all_org_data(self, start_date, end_date, main_dir , refined_dir,organisation_name = None, override_save=False):\n","        \"\"\"\n","        For each date between start_date and end_date, either download\n","        and clean the data or load the pre-saved data. Save the day's data\n","        in case of future use (so it doesn't have to be downloaded and cleaned again)\n","        \"\"\"\n","        print(\"Loading and cleaning all data\")\n","        data_list = []\n","        \n","\n","        # Download and format the daily data\n","        for i, date in enumerate(pd.date_range(start_date, end_date).astype(str)):\n","            if i % 7 == 1:\n","                # Prevent it hanging like it does sometimes\n","                time.sleep(60)\n","\n","            try:\n","                #file_path = os.path.join(main_dir, date)\n","                file_path = os.path.join(main_dir, date)\n","                file_path_refined = os.path.join(refined_dir, date)\n","                df = self.redesign_sdf(self.download_and_generate_gdelt_table(date, file_path), file_path_refined, organisation_name)      \n","                data_list.append(df)\n","                del df\n","                spark.catalog.clearCache()\n","\n","            except Exception as e:\n","              print(f\"!!! Failed to complete {date}!\")\n","              print(\"  ****   Reason:\\n\" + str(e) + \"\\n\\n\")\n","\n","        return combine_spark_dfs(data_list)\n","#     def reformat_sdf(self, sdf, file_path_refined, org_name):\n","#         \"\"\"\n","#         Given a spark data frame of the downloaded data, reformat it\n","#         into human-readable Common_MetaData.\n","#         Add a few more Common_MetaData for our purposes.\n","#         \"\"\"\n","#         sdf = sdf.select(*Common_MetaData.keep)\n","        \n","#         if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n","#             # Reformat existing columns\n","#             sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n","#                       .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n","#                       .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n","#                       .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n","#                    )\n","\n","#             # Create ESG columns & explode organization column\n","#             sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n","#                       .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n","#                       .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n","#                    )\n","        \n","#             sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n","#             print(\"sdf created\")\n","        \n","#         else:\n","#             sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n","        \n","#         sdf = (sdf.withColumn(\"Organization\", F.explode(\"Organizations\"))\n","#                   .withColumn(\"Organization\", clean_organization(\"organization\"))\n","#                   .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n","#               )\n","\n","#         return sdf\n","#      def preprocess_gdelt_data(self ,start_date, end_date, org_name = None, save_csv=True):\n","#         \"\"\"\n","#         \"\"\"   \n","#         dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n","\n","#         base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n","#         if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","#             dbutils.fs.mkdirs(base_dir)\n","\n","#         org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n","#         if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","#             dbutils.fs.mkdirs(org_dir)\n","\n","#         base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n","#         base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n","\n","#         if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","#             dbutils.fs.mkdirs(base_data_dir)\n","#         if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n","#             dbutils.fs.mkdirs(base_data_dir_refined)\n","#         # Download and reformat the data\n","#         print('')\n","#         data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n","#         print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n","#               f\"organizations from {start_date} to {end_date}\")\n","\n","#         # Save the data\n","#         print(\"Saving Data...\")\n","#         data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n","#         if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","#             dbutils.fs.mkdirs(base_data_dir)\n","#         data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n","#         print(f\"Saved to {data_save_path}\")\n","\n","#         return data\n","\n","\n","\n","#     # COMMAND ----------\n","\n","def download_gdelt_data(start_date, end_date, org_name = None, save_csv=True):\n","    \"\"\"\n","    \"\"\"   \n","    dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n","\n","    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n","    if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","        dbutils.fs.mkdirs(base_dir)\n","\n","    org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n","    if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","        dbutils.fs.mkdirs(org_dir)\n","\n","    base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n","    base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n","\n","    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","        dbutils.fs.mkdirs(base_data_dir)\n","    if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n","        dbutils.fs.mkdirs(base_data_dir_refined)\n","    # Download and reformat the data\n","    print('')\n","    data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n","    print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n","          f\"organizations from {start_date} to {end_date}\")\n","\n","    # Save the data\n","    print(\"Saving Data...\")\n","    data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n","    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n","        dbutils.fs.mkdirs(base_data_dir)\n","    data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n","    print(f\"Saved to {data_save_path}\")\n","\n","    return data\n"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b143a2ba-b485-4650-a335-14d60bceeb7c","showTitle":false,"title":""}},"source":["## ESG Computating Functions:"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"deec22cd-7448-4f8e-a74c-d966fc7e21bb","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def singleSource(filtered_df):\n","    #Tone\tPositiveTone\tNegativeTone\tPolarity\tActivityDensity\tSelfDensity\tWordCount\n","    filtered_df_result = (filtered_df.groupby(F.date_format(\"DATE\", \"y-MM-dd\").alias(\"date\"), 'SourceCommonName')\n","                                      .agg(F.mean(\"Tone\"), F.mean(\"PositiveTone\"), F.mean(\"NegativeTone\"), \n","                                           F.mean(\"Polarity\"), F.mean(\"ActivityDensity\"), \n","                                           F.mean(\"SelfDensity\"), F.mean(\"WordCount\"))\n","                                      .withColumn(\"date\", F.to_date(\"date\", format=\"y-MM-dd\"))\n","                                      .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n","                                      .orderBy(F.col(\"date\").asc())\n","                          )\n","    filtered_df_result = (filtered_df_result.select(\"date\", \"SourceCommonName\", F.col(\"avg(Tone)\").alias(\"Tone\"), F.col(\"avg(PositiveTone)\").alias(\"PositiveTone\"), \n","                                             F.col(\"avg(NegativeTone)\").alias(\"NegativeTone\"),\n","                                             F.col(\"avg(Polarity)\").alias(\"Polarity\"), F.col(\"avg(ActivityDensity)\").alias(\"ActivityDensity\"), \n","                                             F.col(\"avg(SelfDensity)\").alias(\"SelfDensity\"), F.col(\"avg(WordCount)\").alias(\"WordCount\"))\n","                                           \n","                          )\n","    return filtered_df_result\n","\n","\n","def avg_day_tone(filtered_df, name):\n","    \"\"\" \"\"\"\n","    colname = f\"{name.replace(' ', '_')}_tone\"\n","    tone_df = (filtered_df.groupby(date_alignment(\"DATE\", \"y-MM-dd\").alias(\"date\"))\n","                          .agg((F.sum(\"Tone\") / F.sum(\"WordCount\")).alias(colname))\n","                          .select(\"date\", f\"{colname}\")\n","                          .withColumn(\"date\", beginning_date(\"date\", format=\"y-MM-dd\"))\n","                          .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n","                          .orderBy(F.col(\"date\").asc())\n","              )\n","    return tone_df\n","\n","# def subtract_cols(df, col1, col2):                                              # call in make_tables \n","\n","#     df = (df.withColumn(col1, df[f\"{col1}\"] - df[f\"{col2}\"])\n","#             .withColumnRenamed(col1, col1.replace(\"_tone\", \"_diff\")))\n","#     return df\n","\n","\n","def get_col_avgs(df):\n","    exclude = [k for k, v in df.dtypes if v in [\"date\", \"timestamp\", \"string\", \"SourceCommonName\"]]\n","    avgs = df.select([F.avg(c).alias(c) for c in df.columns if c not in exclude]).collect()[0]\n","    return {c: avgs[c] for c in df.columns if c not in exclude}\n","\n","# # COMMAND ----------\n","\n","# def show_df_rounded(df, places=4, rows=20):\n","#     dtypes = {k: v for k, v in df.dtypes}\n","#     date_cols = [k for k, v in dtypes.items() if v in [\"date\", \"timestamp\"]]\n","#     str_cols = [k for k, v in dtypes.items() if v == \"string\"]\n","#     int_cols = [k for k, v in dtypes.items() if \"int\" in v]\n","    \n","#     show_cols = [F.date_format(c, \"y-MM-dd\").alias(c) if c in date_cols\n","#                  else (F.col(c).alias(c) if c in str_cols\n","#                  else (F.format_number(c, 0).alias(c) if c in int_cols\n","#                  else (F.format_number(c, places).alias(c))))\n","#                  for c in df.columns]\n","#     show_cols = [c for c in show_cols]\n","#     df.select(*show_cols).limit(rows).show()\n","\n","# # DBTITLE 1,Load Data from Delta Table\n","# def load_data(save_path, file_name): \n","#   df = (spark.read.format(\"delta\")\n","#                       .option(\"header\", \"true\")\n","#                       .option(\"inferSchema\", \"true\")\n","#                       .load(os.path.join(save_path, file_name))\n","#            )\n","#   return df.toPandas()\n","\n","\n","def filter_non_esg(df): \n","    return df[(df['E']==True) | (df['S'] == True) | (df['G'] == True)]\n","\n","# COMMAND ----------\n","\n","class graph_creator:\n","    def __init__(self, df):\n","        self.df = df\n","\n","    def create_graph(self):\n","        # Find Edges\n","        df_edge = pd.DataFrame(self.df.groupby(\"URL\").Organization.apply(list)\n","                               ).reset_index()\n","\n","        get_tpls = lambda r: (list(itertools.combinations(r, 2)) if\n","                              len(r) > 1 else None)\n","        df_edge[\"SourceDest\"] = df_edge.Organization.apply(get_tpls)\n","        df_edge = df_edge.intelora_data_aligner(\"SourceDest\").to_clean_require(subset=[\"SourceDest\"])\n","\n","        # Get Weights\n","        source_dest = pd.DataFrame(df_edge.SourceDest.tolist(),\n","                                   columns=[\"Source\", \"Dest\"])\n","        sd_mapping = source_dest.intelora.arranger([\"Source\", \"Dest\"]).size()\n","        get_weight = lambda r: sd_mapping[r.Source, r.Dest]\n","        source_dest[\"weight\"] = source_dest.intelora_add(get_weight, axis=1)\n","\n","        # Get\n","        self.organizations = set(source_dest.Source.unique()).union(\n","                             set(source_dest.Dest.unique()))\n","        self.G = nx.from_pandas_edgelist(source_dest, source=\"Source\",\n","            target=\"Dest\", edge_attr=\"weight\", create_using=nx.Graph)\n","        return self.G\n","\n","# # COMMAND ----------\n","\n","def get_embeddings(G, organizations):\n","    # Fit graph\n","    g2v = NVVV()\n","    g2v.fit(G)\n","    \n","    # Embeddings\n","    embeddings = g2v.model.wv.vectors\n","    pca = PCA(n_components=3)\n","    principalComponents = pca.fit_transform(embeddings)\n","    d_e = pd.DataFrame(principalComponents)\n","    d_e[\"company\"] = organizations\n","    return d_e, g2v\n","\n","# COMMAND ----------\n","\n","def get_connections(organizations, topn=25):\n","    l = [g2v.model.wv.most_similar(org, topn=topn)\n","         for org in organizations]\n","    df_sim = pd.DataFrame(l, columns=[f\"n{i}\" for i in range(topn)])\n","    for col in df_sim.columns:\n","        new_cols = [f\"{col}_rec\", f\"{col}_conf\"]\n","        df_sim[new_cols] = pd.DataFrame(df_sim[col].tolist(), \n","                                        index=df_sim.index)\n","    df_sim = df_sim.drop(columns=[f\"n{i}\" for i in range(topn)])\n","    df_sim.insert(0, \"company\", list(organizations))\n","    return df_sim\n","\n","# # COMMAND ----------\n","\n","def make_embeddings_and_connections(start, end):\n","    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_Russell_top_300\"\n","    save_dir = os.path.join(base_dir, f\"{start}__to__{end}\")\n","    csv_file = \"data_as_csv.csv\"\n","\n","    # Load data\n","    print(\"Loading Data\")\n","    df = pd.read_csv(os.path.join(save_dir, csv_file).replace(\"dbfs:/\", \"/dbfs/\"))\n","    df = filter_non_esg(df)\n","\n","    # Create graph\n","    print(\"Creating Graph\")\n","    creator = graph_creator(df)\n","    G = creator.create_graph()\n","    organizations = list(creator.organizations)\n","\n","    # Save graph as pkl\n","    fp = os.path.join(save_dir, \"organization_graph.pkl\").replace(\"dbfs:/\", \"/dbfs/\")\n","    with open(fp, \"wb\") as f:\n","        pickle.dump(G, f)\n","        \n","    # Create embeddings\n","    print(\"Creating embeddings\")\n","    emb_path = os.path.join(save_dir, \"pca_embeddings.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n","    d_e, g2v = get_embeddings(G, organizations)\n","    d_e.to_csv(emb_path, index=False)\n","    \n","    # Create connections\n","    print(\"Creating connections\")\n","    df_sim = get_connections(organizations)\n","    sim_path = os.path.join(save_dir, \"connections.csv\")\n","    df_sim.to_csv(sim_path.replace(\"dbfs:/\", \"/dbfs/\"))\n","    \n","    # Save organizations as delta\n","    conn_path = os.path.join(save_dir, \"CONNECTIONS\")\n","    conn_data = spark.createDataFrame(df_sim)\n","    conn_data.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\").save(conn_path)\n","    \n","# def make_tables(start_date, end_date):\n","#      \"\"\"\n","#      \"\"\"\n","#      # Directories\n","#      org_types = f\"Russell_top_{Fields.n_orgs}\"\n","#      base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_{org_types}\"\n","#      range_save_dir = os.path.join(base_dir, f\"{start_date}__to__{end_date}\")\n","#      esg_dir = os.path.join(range_save_dir, \"esg_scores\")\n","#      dbutils.fs.mkdirs(esg_dir)\n","    \n","#      # Load data\n","#      data_path = os.path.join(range_save_dir, \"data_as_delta\")\n","#      try:\n","#          data = (spark.read.format(\"delta\")\n","#                       .option(\"header\", \"true\")\n","#                       .option(\"inferSchema\", \"true\")\n","#                       .load(data_path)\n","#                 )\n","#          print(\"Data Loaded!\")\n","#      except:\n","#          print(\"Data for these dates hasn't been generated!!!\")\n","#          return\n","\n","#     # Get all organizations\n","#     print(\"Finding all Organizations\")\n","#     organizations = [x.Organization for x in data.select(\n","#                      \"Organization\").distinct().collect()]\n","    \n","#     # Get the overall tone\n","#     print(\"Calculating Tones Over Time\")\n","#     overall_tone = daily_tone(data, \"industry\")\n","#     esg_tones = {L: daily_tone(data.filter(f\"{L} == True\"), \"industry\")\n","#                  for L in [\"E\", \"S\", \"G\"]}\n","    \n","#     # Loop through the organizations to get the average daily tone for each company\n","#     pct_idxs = range(0, len(organizations), len(organizations) // 10)\n","#     for i, org in enumerate(organizations):\n","#         if i in pct_idxs:\n","#             print(f\"{pct_idxs.index(i) * 10}%\")\n","#         tone_label = f\"{org.replace(' ', '_')}_tone\"\n","        \n","#         overall_org_df = data.filter(f\"Organization == '{org}'\")\n","#         org_tone = daily_tone(overall_org_df, org)\n","#         overall_tone = subtract_cols(overall_tone.join(org_tone, on=\"date\", how=\"left\"),\n","#                                      tone_label, \"industry_tone\")\n","      \n","#         for L, tdf in esg_tones.items():\n","#             esg_org_df = overall_org_df.filter(f\"{L} == True\")\n","#             esg_org_tone = daily_tone(esg_org_df, org)\n","#             esg_tones[L] = subtract_cols(tdf.join(esg_org_tone, on=\"date\", how=\"left\"), \n","#                                          tone_label, \"industry_tone\")            \n","#     del data   \n","    \n","#     # Average to get overall scores\n","#     print(\"Computing Overall Scores\")\n","#     scores = {}\n","#     overall_scores = get_col_avgs(overall_tone)\n","#     esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n","\n","#     for org in organizations:\n","#         diff_label = f\"{org.replace(' ', '_')}_diff\"\n","#         scores[org] = {L: tdf[diff_label] for L, tdf in esg_scores.items()}\n","#         scores[org][\"T\"] = overall_scores[diff_label]\n","      \n","      \n","#     # Save all the tables\n","#     print(\"Saving Tables\")  \n","    \n","#     # Overall ESG\n","#     print(\"    Daily Overall ESG\")\n","#     path = os.path.join(esg_dir, \"overall_daily_esg_scores.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n","#     pd_df = overall_tone.toPandas().set_index(\"date\").sort_index().asfreq(freq=\"D\", method=\"ffill\")\n","#     pd_df.to_csv(path, index=True)\n","    \n","#     # E, S, and G\n","#     for L, tdf in esg_tones.items():\n","#         print(\"    Daily \" + L)\n","#         path = os.path.join(esg_dir, f\"daily_{L}_score.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n","#         pd_df = tdf.toPandas().set_index(\"date\").sort_index().asfreq(freq=\"D\", method=\"ffill\")\n","#         pd_df.to_csv(path, index=True)\n","\n","#     # Averaged scores\n","#     print(\"    Average Scores\")\n","#     score_path = path = os.path.join(esg_dir, \"average_esg_scores.csv\").replace(\"dbfs:/\", \"/dbfs/\")\n","#     pd.DataFrame(scores).to_csv(score_path, index=True)\n","#     print(\"DONE!\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b260a08a-b986-4300-b39a-4a0bb72d5cd1","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def calculating_esg_values(start_date, end_date):\n","    \"\"\"\n","    \"\"\"\n","    print(\"calulation started\")\n","    data_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\"\n","    try:\n","        data = (spark.read.format(\"delta\")\n","                     .option(\"header\", \"true\")\n","                     .option(\"inferSchema\", \"true\")\n","                     .load(data_path)\n","               )\n","        print(\"Data Loaded!\")\n","    except:\n","        print(\"Data for these dates hasn't been generated!!!\")\n","        return\n","\n","    # Get all organizations\n","    print(\"Finding all Organizations\")\n","    organizations = [x.Organization for x in data.select(\n","                     \"Organization\").distinct().collect()]\n","    \n","    # Get the overall tone\n","    print(organizations)\n","    print(\"Calculating Tones Over Time\")\n","    overall_tone = avg_day_tone(singleSource(data), \"industry\")                                              \n","    esg_tones = {L: avg_day_tone(singleSource(data.filter(f\"{L} == True\")), \"industry\")\n","                 for L in [\"E\", \"S\", \"G\"]}\n","    \n","    pct_idxs = range(0, len(organizations))\n","    \n","    for i, org in enumerate(organizations):\n","        tone_label = f\"{org.replace(' ', '_')}_tone\"\n","\n","        overall_org_df = data.filter(f\"Organization == '{org}'\")\n","        org_tone = avg_day_tone(singleSource(overall_org_df), org)\n","\n","        overall_tone = overall_tone.join(org_tone, on=\"date\", how=\"left\")\n","      \n","        for L, tdf in esg_tones.items():\n","            esg_org_df = overall_org_df.filter(f\"{L} == True\")\n","            esg_org_tone = avg_day_tone(singleSource(esg_org_df), org)\n","\n","            \n","            esg_tones[L] = tdf.join(esg_org_tone, on=\"date\", how=\"left\")\n","    \n","    \n","         \n","    del data \n","    \n","    iord_consult = \"True\"\n","    if iord_consult == \"false\": \n","        make_embeddings_and_connections(start_date,end_date)\n","    else:\n","        # Average to get overall scores\n","        print(organizations)\n","        print(\"Computing Overall Scores\")\n","        scores = {}\n","        print(\"    Calculating overall tone\")\n","        overall_scores = get_col_avgs(overall_tone)\n","        print(\"    Calculating esg tone\")\n","        esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n","    \n","        #print(overall_scores)\n","        print(esg_scores)\n","        print(\"DONE!\")\n","        return esg_scores, overall_tone\n","        \n","    \n","    \n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"06496dbe-0dad-4ceb-b16c-8340047e555a","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\"></div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\"></div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["def download_company_data(base_dir, start_date, end_date, org_name):\n","    #print(org_name)\n","    _ = download_gdelt_data(start_date, end_date,org_name, save_csv=True)\n","    #print(org_name)\n","    esg_score1, _ = calculating_esg_values(start_date, end_date)\n","    return esg_score1"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"3bbb3b2c-cc98-402b-946f-4c331f6e105c","showTitle":false,"title":""}},"source":["### Calculating ESG Scores for input Companies:"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2a1f7576-20f7-48bd-b784-c87c45eb7a7b","showTitle":false,"title":""}},"outputs":[{"data":{"text/html":["<style scoped>\n","  .ansiout {\n","    display: block;\n","    unicode-bidi: embed;\n","    white-space: pre-wrap;\n","    word-wrap: break-word;\n","    word-break: break-all;\n","    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n","    font-size: 13px;\n","    color: #555;\n","    margin-left: 4px;\n","    line-height: 19px;\n","  }\n","</style>\n","<div class=\"ansiout\">[&#39;logitech&#39;]\n","</div>"]},"metadata":{"application/vnd.databricks.v1+output":{"addedWidgets":{},"arguments":{},"data":"<div class=\"ansiout\">[&#39;logitech&#39;]\n</div>","datasetInfos":[],"metadata":{},"removedWidgets":[],"type":"html"}},"output_type":"display_data"}],"source":["base_dir = \"dbfs:/mnt/esg/financial_report_data\" \n","dbutils.widgets.text(\"myinput\",\"microsoft;apple\")\n","dbutils.widgets.text(\"startdate\",\"2022-05-01\")\n","dbutils.widgets.text(\"enddate\",\"2022-05-02\")\n","\n","var_a = dbutils.widgets.get(\"myinput\")\n","var_a = var_a.split(';')\n","print(var_a)\n","start_date = dbutils.widgets.get(\"startdate\")\n","end_date = dbutils.widgets.get(\"enddate\")"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"61e8c5fc-2dfe-4918-8917-34243d699e40","showTitle":false,"title":""}},"outputs":[{"data":{"text/plain":["{'E': {'industry_tone': -0.0012835787906290864, 'logitech_tone': -0.0012835787906290864}, 'S': {'industry_tone': 0.0012170680784442736, 'logitech_tone': 0.0012170680784442736}, 'G': {'industry_tone': 0.0022472222767447444, 'logitech_tone': 0.0022472222767447444}}"]},"metadata":{"application/vnd.databricks.v1+output":{"arguments":{},"data":"{'E': {'industry_tone': -0.0012835787906290864, 'logitech_tone': -0.0012835787906290864}, 'S': {'industry_tone': 0.0012170680784442736, 'logitech_tone': 0.0012170680784442736}, 'G': {'industry_tone': 0.0022472222767447444, 'logitech_tone': 0.0022472222767447444}}","metadata":{},"type":"exit"}},"output_type":"display_data"}],"source":["output = download_company_data(base_dir, start_date, end_date, var_a)\n","dbutils.notebook.exit(output)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9590e05e-fc1f-4335-ae5b-c4b4da5d16f6","showTitle":false,"title":""}},"outputs":[],"source":[]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":-1,"dataframes":["_sqldf"]},"pythonIndentUnit":4},"notebookName":"esg_final_cleaned","notebookOrigID":1862777018676218,"widgets":{"enddate":{"currentValue":"2022-05-05","nuid":"952f80be-0341-4f67-911b-08855fa315b7","widgetInfo":{"defaultValue":"2022-05-02","label":null,"name":"enddate","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"myinput":{"currentValue":"logitech","nuid":"d665e895-3192-4cd3-bdca-caf8ac4bc19b","widgetInfo":{"defaultValue":"microsoft;apple","label":null,"name":"myinput","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}},"startdate":{"currentValue":"2022-05-01","nuid":"fcdcf192-5ee6-41ab-bf7d-6d10630f4034","widgetInfo":{"defaultValue":"2022-05-01","label":null,"name":"startdate","options":{"validationRegex":null,"widgetType":"text"},"widgetType":"text"}}}},"kernelspec":{"display_name":"Python 3.9.7 64-bit (microsoft store)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"vscode":{"interpreter":{"hash":"bf9f081df7a9fb6e3e60ce586fd6298abdfbef8d0b52c7e06ae6a4386281fc66"}}},"nbformat":4,"nbformat_minor":0}
