{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Required Packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "from collections import Counter\n",
    "from pprint import pprint\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import DataFrame as spark_DataFrame\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce\n",
    "import matplotlib.pyplot as plt\n",
    "import requests\n",
    "import re\n",
    "from itertools import product\n",
    "import time\n",
    "from io import BytesIO\n",
    "import requests\n",
    "import numpy as np\n",
    "from pyspark.sql import Window\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def also_call_as(c):\n",
    "    \"\"\"\n",
    "    Make a list of aliases for a given company using a few predefined\n",
    "    rules. Try to encompass as many options as possible keeping in mind\n",
    "    there will be aliases left out.\n",
    "    \"\"\"\n",
    "    c = c.lower()\n",
    "\n",
    "    # URLs (e.g. \".com\")\n",
    "    if len(c) > 3 and c[-4] == \".\":\n",
    "        a = c\n",
    "        c = c.rsplit(\".\", 1)[0].replace(\".\", \" \")\n",
    "        aliases = set([c, a])\n",
    "    else:\n",
    "        aliases = set([c])\n",
    "\n",
    "    # Single letter endings\n",
    "    if len(c.split()[-1]) == 1:\n",
    "        c = c.rsplit(\" \", 1)[0]\n",
    "        aliases.add(c)\n",
    "\n",
    "    # Company legal endings\n",
    "    \n",
    "    endings = [\"inc\",\n",
    "               \"corp\",\n",
    "               \"plc\", \n",
    "               \"reit\", \n",
    "               \"co\", \n",
    "               \"cor\", \n",
    "               \"group\", \n",
    "               \"company\",\n",
    "               \"trust\",\n",
    "               \"energy\",\n",
    "               \"international\",\n",
    "               \"of america\",\n",
    "               \"pharmaceuticals\",\n",
    "               \"clas\",\n",
    "               \"in\", \"nv\",\n",
    "               \"sa\", \n",
    "               \"re\", \n",
    "               \"pvt ltd\",\n",
    "               \"private limited\" ,\n",
    "               \"india private limited\"]\n",
    "    \n",
    "    n_endings = 3  # Can have up to 3 of these endings\n",
    "    for _ in range(n_endings):\n",
    "        aliases.update([a.rsplit(\" \", 1)[0] for a in aliases if\n",
    "                        any([a.endswith(\" \" + e) for e in endings])])\n",
    "        c = c.rsplit(\" \", 1)[0] if any([c.endswith(\" \" + e) for e in endings]) else c\n",
    "\n",
    "    # Alias any dashes and replace in company name\n",
    "    aliases.update([a.replace(\"-\", \"\") for a in aliases] +\n",
    "                   [a.replace(\"-\", \" \") for a in aliases])\n",
    "    c = c.replace(\"-\", \" \")\n",
    "\n",
    "    # If '&' stands on its own, add alias of 'and'\n",
    "    aliases.update([a.replace(\" & \", \" and \") for a in aliases])\n",
    "\n",
    "    return {c: list(aliases)}\n",
    "\n",
    "\n",
    "def generate_common_org_names(companies):\n",
    "    \"\"\"\n",
    "    Download the companies and loop through them to find their aliases\n",
    "    \"\"\"\n",
    "    #companies = [org_name]\n",
    "    comp_dict = dict()\n",
    "    for c in companies:\n",
    "        comp_dict.update(also_call_as(c))\n",
    "    return comp_dict\n",
    "\n",
    "\n",
    "combine_spark_dfs = lambda sdf_list: reduce(spark_DataFrame.union, sdf_list)\n",
    "\n",
    "class Common_MetaData:\n",
    "    \"\"\" Variables to use across many functions. \"\"\"\n",
    "    keep = [\"DATE\",\n",
    "            \"SourceCommonName\", \n",
    "            \"DocumentIdentifier\", \n",
    "            \"Themes\",\n",
    "            \"Organizations\",\n",
    "            \"V2Tone\"]\n",
    "    tone = [\"Tone\",\n",
    "            \"PositiveTone\",\n",
    "            \"NegativeTone\", \n",
    "            \"Polarity\",\n",
    "            \"ActivityDensity\", \n",
    "            \"SelfDensity\",\n",
    "            \"WordCount\"]\n",
    "    organizations = None\n",
    "\n",
    "\n",
    "\n",
    "@udf(ArrayType(StringType(), True))\n",
    "def simple_expand_spark(x):\n",
    "    \"\"\" Expand a semicolon separated strint to a list (ignoring empties)\"\"\"\n",
    "    if not x:\n",
    "        return []\n",
    "    return list(filter(None, x.split(\";\")))\n",
    "\n",
    "\n",
    "\n",
    "@udf(MapType(StringType(), DoubleType()))\n",
    "def tone_expand_spark(x):\n",
    "    \"\"\" Expand the tone field. \"\"\"\n",
    "    if not x:\n",
    "        return {t: None for t in Common_MetaData.tone}\n",
    "    return {Common_MetaData.tone[i]: float(v) for i, v in enumerate(x.split(\",\"))}\n",
    "\n",
    "\n",
    "\n",
    "@udf(BooleanType())\n",
    "def has_theme_spark(x, theme):\n",
    "    \"\"\" Is the given theme included in any of the listed themes? \"\"\"\n",
    "    return any([theme in lst.split(\"_\") for lst in x])\n",
    "\n",
    "\n",
    "@udf(StringType())\n",
    "def clean_organization(s):\n",
    "    \"\"\" Standardize the organization names. \"\"\"\n",
    "    for k, v in Common_MetaData.organizations.items():\n",
    "        if v[0] in s.split() :\n",
    "            return k\n",
    "    return s.lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading , Filtering and Storing Gdelt Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ed691b2-9044-4bd4-ba48-3d72ef2052d4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "class Redesign_data_format:\n",
    "    def redesign_sdf(self, sdf, file_path_refined, org_name):\n",
    "        \"\"\"\n",
    "        Given a spark data frame of the downloaded data, reformat it\n",
    "        into human-readable Common_MetaData.\n",
    "        Add a few more Common_MetaData for our purposes.\n",
    "        \"\"\"\n",
    "        setattr(Common_MetaData, \"organizations\",generate_common_org_names(org_name))\n",
    "        \n",
    "        sdf = sdf.select(*Common_MetaData.keep)\n",
    "        \n",
    "        if not os.path.exists(file_path_refined.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "            # Reformat existing columns\n",
    "            sdf = (sdf.withColumnRenamed(\"DocumentIdentifier\", \"URL\")\n",
    "                      .withColumn(\"Themes\", simple_expand_spark(\"Themes\"))\n",
    "                      .withColumn(\"Organizations\", simple_expand_spark(\"Organizations\"))\n",
    "                      .withColumn(\"V2Tone\", tone_expand_spark(\"V2Tone\"))\n",
    "                   )\n",
    "\n",
    "            # Create ESG columns & explode organization column\n",
    "            sdf = (sdf.withColumn(\"E\", has_theme_spark(\"Themes\", F.lit(\"ENV\")))\n",
    "                      .withColumn(\"S\", has_theme_spark(\"Themes\", F.lit(\"UNGP\")))\n",
    "                      .withColumn(\"G\", has_theme_spark(\"Themes\", F.lit(\"ECON\")))                      \n",
    "                   )\n",
    "        \n",
    "            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path_refined)\n",
    "            print(\"sdf created\")\n",
    "        \n",
    "        else:\n",
    "            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path_refined)    \n",
    "        \n",
    "        sdf = (sdf.withColumn(\"Organization\", F.explode(\"Organizations\"))\n",
    "                  .withColumn(\"Organization\", clean_organization(\"organization\"))\n",
    "                  .filter(F.col(\"organization\").isin(list(Common_MetaData.organizations.keys())))\n",
    "              )\n",
    "\n",
    "        # Expand tone columns\n",
    "        exprs = [F.col(\"V2Tone\").getItem(k).alias(k) for k in Common_MetaData.tone]\n",
    "\n",
    "        sdf = sdf.select(*sdf.columns, *exprs).drop(\"V2Tone\")\n",
    "        #print(sdf.column)\n",
    "        return sdf\n",
    "\n",
    "\n",
    "    def download_and_generate_gdelt_table1(self, date, gd):\n",
    "        \"\"\"\n",
    "        Download the GDELT table as a pandas dataframe using the gdelt package.\n",
    "        Return a spark data frame.\n",
    "        \"\"\"\n",
    "        pdf = gd.Search([date], table=\"gkg\",coverage=True, output=\"df\")\n",
    "        pdf[\"DATE\"] = pd.to_datetime(pdf[\"DATE\"], format=\"%Y%m%d%H%M%S\")\n",
    "\n",
    "        sdf = spark.createDataFrame(pdf)\n",
    "        print(\"   * loaded *  \", date)\n",
    "        return sdf\n",
    "    \n",
    "    def download_and_generate_gdelt_table(self, date, file_path):\n",
    "        file_path_os =  file_path.replace(\"dbfs:/\", \"/dbfs/\")\n",
    "        s = []\n",
    "        for i in range(24):\n",
    "            for j in list(range(0, 60, 15)):\n",
    "                if i<10:\n",
    "                    if j<10:\n",
    "                        s.append('0' + str(i) + '0' + str(j))\n",
    "                    else:\n",
    "                        s.append('0' + str(i) + str(j))\n",
    "                else:\n",
    "                    if j<10:\n",
    "                        s.append(str(i) + '0' + str(j))\n",
    "                    else:\n",
    "                         s.append(str(i) + str(j))\n",
    "                            \n",
    "        if not os.path.exists(file_path_os):\n",
    "            li = ''.join(date.split('-'))\n",
    "            c=0\n",
    "            df1 = None\n",
    "            for elem in s:\n",
    "                try:\n",
    "                    print(li, elem)\n",
    "                    response = requests.get('http://data.gdeltproject.org/gdeltv2/'+li+ elem + '00.gkg.csv.zip')\n",
    "                    #'http://data.gdeltproject.org/gkg/20220204.gkg.csv.zip')\n",
    "                    buffer = BytesIO(response.content)\n",
    "                   \n",
    "                    frame = pd.read_csv(buffer, compression='zip', sep='\\t',header=None, warn_bad_lines=True,encoding='latin')\n",
    "                    frame[1] = pd.to_datetime(frame[1], format=\"%Y%m%d%H%M%S\")\n",
    "        \n",
    "                    frame.columns = ['GKGRECORDID', \n",
    "                                     'DATE', \n",
    "                                     'SourceCollectionIdentifier', \n",
    "                                     'SourceCommonName',\n",
    "                                     'DocumentIdentifier',\n",
    "                                     'Counts',\n",
    "                                     'V2Counts', \n",
    "                                     'Themes', \n",
    "                                     'V2Themes',\n",
    "                                     'Locations',\n",
    "                                     'V2Locations', \n",
    "                                     'Persons', \n",
    "                                     'V2Persons', \n",
    "                                     'Organizations',\n",
    "                                     'V2Organizations', \n",
    "                                     'V2Tone',\n",
    "                                     'Dates',\n",
    "                                     'GCAM',\n",
    "                                     'SharingImage',\n",
    "                                     'RelatedImages', \n",
    "                                     'SocialImageEmbeds', \n",
    "                                     'SocialVideoEmbeds', \n",
    "                                     'Quotations',\n",
    "                                     'AllNames', \n",
    "                                     'Amounts', \n",
    "                                     'TranslationInfo', \n",
    "                                     'Extras'] \n",
    "                    columns1 = ['DATE', \n",
    "                                'SourceCollectionIdentifier', \n",
    "                                'SourceCommonName',\n",
    "                                'DocumentIdentifier',\n",
    "                                'Counts', \n",
    "                                'V2Counts',\n",
    "                                'Themes',\n",
    "                                'V2Themes',\n",
    "                                'Locations',\n",
    "                                'V2Locations',\n",
    "                                'Organizations',\n",
    "                                'V2Organizations',\n",
    "                                'V2Tone',\n",
    "                                'Dates'] \n",
    "                    frame = frame[columns1]\n",
    "                    print(frame.shape)\n",
    "                    if c==0:\n",
    "                        df1 = frame\n",
    "                        c=1\n",
    "                    else:\n",
    "                        df1 = df1.append(frame, ignore_index=True)\n",
    "                except:\n",
    "                    pass\n",
    "            \n",
    "            sdf = spark.createDataFrame(df1)\n",
    "            sdf.write.format(\"delta\").option(\"header\", \"true\").mode(\"overwrite\").save(file_path)\n",
    "            print(\"   * loaded *  \", date)\n",
    "        else:\n",
    "            sdf = spark.read.format(\"delta\").option(\"header\", \"true\").load(file_path)\n",
    "        return sdf\n",
    "\n",
    "\n",
    "    def getting_all_org_data(self, start_date, end_date, main_dir , refined_dir,organisation_name = None, override_save=False):\n",
    "        \"\"\"\n",
    "        For each date between start_date and end_date, either download\n",
    "        and clean the data or load the pre-saved data. Save the day's data\n",
    "        in case of future use (so it doesn't have to be downloaded and cleaned again)\n",
    "        \"\"\"\n",
    "        print(\"Loading and cleaning all data\")\n",
    "        data_list = []\n",
    "\n",
    "        # Download and format the daily data\n",
    "        for i, date in enumerate(pd.date_range(start_date, end_date).astype(str)):\n",
    "            if i % 7 == 1:\n",
    "                # Prevent it hanging like it does sometimes\n",
    "                time.sleep(60)\n",
    "\n",
    "            try:\n",
    "                file_path = os.path.join(main_dir, date)\n",
    "                file_path_refined = os.path.join(refined_dir, date)\n",
    "                df = self.redesign_sdf(self.download_and_generate_gdelt_table(date, file_path), file_path_refined, organisation_name)      \n",
    "                data_list.append(df)\n",
    "                del df\n",
    "                spark.catalog.clearCache()\n",
    "\n",
    "            except Exception as e:\n",
    "              print(f\"!!! Failed to complete {date}!\")\n",
    "              print(\"  ****   Reason:\\n\" + str(e) + \"\\n\\n\")\n",
    "\n",
    "        return combine_spark_dfs(data_list)\n",
    "\n",
    "# COMMAND ----------\n",
    "\n",
    "def download_gdelt_data(start_date, end_date, org_name = None, save_csv=True):\n",
    "    \"\"\"\n",
    "    \"\"\"   \n",
    "    dbutils.fs.rm('/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org', True)    \n",
    "\n",
    "    base_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org\"\n",
    "    if not os.path.exists(base_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(base_dir)\n",
    "        \n",
    "    org_dir = 'dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org'\n",
    "    if not os.path.exists(org_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(org_dir)\n",
    "    \n",
    "    base_data_dir = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data\"\n",
    "    base_data_dir_refined = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_refined\"\n",
    "    \n",
    "    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(base_data_dir)\n",
    "    if not os.path.exists(base_data_dir_refined.replace(\"dbfs:/\", \"/dbfs/\")):     \n",
    "        dbutils.fs.mkdirs(base_data_dir_refined)\n",
    "    # Download and reformat the data\n",
    "    print('')\n",
    "    data = Redesign_data_format().getting_all_org_data(start_date, end_date, base_data_dir, base_data_dir_refined, org_name)\n",
    "    print(f\"There are {data.count():,d} data points for {len(Common_MetaData.organizations)} \"\n",
    "          f\"organizations from {start_date} to {end_date}\")\n",
    "\n",
    "    # Save the data\n",
    "    print(\"Saving Data...\")\n",
    "    data_save_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\" ## we are changing\n",
    "    if not os.path.exists(base_data_dir.replace(\"dbfs:/\", \"/dbfs/\")):\n",
    "        dbutils.fs.mkdirs(base_data_dir)\n",
    "    data.write.format(\"delta\").mode(\"overwrite\").save(data_save_path) ## need to remove this..\n",
    "    print(f\"Saved to {data_save_path}\")\n",
    "\n",
    "    return data\n",
    "\n",
    "# COMMAND ----------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ESG Computating Functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "deec22cd-7448-4f8e-a74c-d966fc7e21bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def singleSource(filtered_df):\n",
    "    #Tone\tPositiveTone\tNegativeTone\tPolarity\tActivityDensity\tSelfDensity\tWordCount\n",
    "    filtered_df_result = (filtered_df.groupby(F.date_format(\"DATE\", \"y-MM-dd\").alias(\"date\"), 'SourceCommonName')\n",
    "                                      .agg(F.mean(\"Tone\"), F.mean(\"PositiveTone\"), F.mean(\"NegativeTone\"), \n",
    "                                           F.mean(\"Polarity\"), F.mean(\"ActivityDensity\"), \n",
    "                                           F.mean(\"SelfDensity\"), F.mean(\"WordCount\"))\n",
    "                                      .withColumn(\"date\", F.to_date(\"date\", format=\"y-MM-dd\"))\n",
    "                                      .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
    "                                      .orderBy(F.col(\"date\").asc())\n",
    "                          )\n",
    "    filtered_df_result = (filtered_df_result.select(\"date\", \"SourceCommonName\", F.col(\"avg(Tone)\").alias(\"Tone\"), F.col(\"avg(PositiveTone)\").alias(\"PositiveTone\"), \n",
    "                                             F.col(\"avg(NegativeTone)\").alias(\"NegativeTone\"),\n",
    "                                             F.col(\"avg(Polarity)\").alias(\"Polarity\"), F.col(\"avg(ActivityDensity)\").alias(\"ActivityDensity\"), \n",
    "                                             F.col(\"avg(SelfDensity)\").alias(\"SelfDensity\"), F.col(\"avg(WordCount)\").alias(\"WordCount\"))\n",
    "                                           \n",
    "                          )\n",
    "    return filtered_df_result\n",
    "\n",
    "\n",
    "def avg_day_tone(filtered_df, name):\n",
    "    \"\"\" \"\"\"\n",
    "    colname = f\"{name.replace(' ', '_')}_tone\"\n",
    "    tone_df = (filtered_df.groupby(F.date_format(\"DATE\", \"y-MM-dd\").alias(\"date\"))\n",
    "                          .agg((F.sum(\"Tone\") / F.sum(\"WordCount\")).alias(colname))\n",
    "                          .select(\"date\", f\"{colname}\")\n",
    "                          .withColumn(\"date\", F.to_date(\"date\", format=\"y-MM-dd\"))\n",
    "                          .withColumn(\"date\", F.col(\"date\").cast(\"date\"))\n",
    "                          .orderBy(F.col(\"date\").asc())\n",
    "              )\n",
    "    return tone_df\n",
    "\n",
    "def subtract_cols(df, col1, col2):\n",
    "    df = (df.withColumn(col1, df[f\"{col1}\"] - df[f\"{col2}\"])\n",
    "            .withColumnRenamed(col1, col1.replace(\"_tone\", \"_diff\")))\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_col_avgs(df):\n",
    "    exclude = [k for k, v in df.dtypes if v in [\"date\", \"timestamp\", \"string\", \"SourceCommonName\"]]\n",
    "    avgs = df.select([F.avg(c).alias(c) for c in df.columns if c not in exclude]).collect()[0]\n",
    "    return {c: avgs[c] for c in df.columns if c not in exclude}\n",
    "\n",
    "# COMMAND ----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculating_esg_values(start_date, end_date):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(\"calulation started\")\n",
    "    data_path = f\"dbfs:/mnt/esg/financial_report_data/GDELT_data_single_org/data_single_org\"\n",
    "    try:\n",
    "        data = (spark.read.format(\"delta\")\n",
    "                     .option(\"header\", \"true\")\n",
    "                     .option(\"inferSchema\", \"true\")\n",
    "                     .load(data_path)\n",
    "               )\n",
    "        print(\"Data Loaded!\")\n",
    "    except:\n",
    "        print(\"Data for these dates hasn't been generated!!!\")\n",
    "        return\n",
    "\n",
    "    # Get all organizations\n",
    "    print(\"Finding all Organizations\")\n",
    "    organizations = [x.Organization for x in data.select(\n",
    "                     \"Organization\").distinct().collect()]\n",
    "    \n",
    "    # Get the overall tone\n",
    "    print(organizations)\n",
    "    print(\"Calculating Tones Over Time\")\n",
    "    overall_tone = avg_day_tone(singleSource(data), \"industry\")\n",
    "    #overall_tone = avg_day_tone(data, \"industry\")\n",
    "    esg_tones = {L: avg_day_tone(singleSource(data.filter(f\"{L} == True\")), \"industry\")\n",
    "                 for L in [\"E\", \"S\", \"G\"]}\n",
    "    \n",
    "    pct_idxs = range(0, len(organizations))\n",
    "    \n",
    "    for i, org in enumerate(organizations):\n",
    "        tone_label = f\"{org.replace(' ', '_')}_tone\"\n",
    "\n",
    "        overall_org_df = data.filter(f\"Organization == '{org}'\")\n",
    "        org_tone = avg_day_tone(singleSource(overall_org_df), org)\n",
    "\n",
    "        overall_tone = overall_tone.join(org_tone, on=\"date\", how=\"left\")\n",
    "      \n",
    "        for L, tdf in esg_tones.items():\n",
    "            esg_org_df = overall_org_df.filter(f\"{L} == True\")\n",
    "            esg_org_tone = avg_day_tone(singleSource(esg_org_df), org)\n",
    "\n",
    "            \n",
    "            esg_tones[L] = tdf.join(esg_org_tone, on=\"date\", how=\"left\")\n",
    "    \n",
    "    \n",
    "         \n",
    "    del data   \n",
    "    \n",
    "    \n",
    "    # Average to get overall scores\n",
    "    print(organizations)\n",
    "    print(\"Computing Overall Scores\")\n",
    "    scores = {}\n",
    "    print(\"    Calculating overall tone\")\n",
    "    overall_scores = get_col_avgs(overall_tone)\n",
    "    print(\"    Calculating esg tone\")\n",
    "    esg_scores = {L: get_col_avgs(tdf) for L, tdf in esg_tones.items()}\n",
    "    \n",
    "    #print(overall_scores)\n",
    "    print(esg_scores)\n",
    "    print(\"DONE!\")\n",
    "    return esg_scores, overall_tone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "06496dbe-0dad-4ceb-b16c-8340047e555a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def download_company_data(base_dir, start_date, end_date, org_name):\n",
    "    #print(org_name)\n",
    "    _ = download_gdelt_data(start_date, end_date,org_name, save_csv=True)\n",
    "    #print(org_name)\n",
    "    esg_score1, _ = calculating_esg_values(start_date, end_date)\n",
    "    return esg_score1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating ESG Scores for input Companies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = \"dbfs:/mnt/esg/financial_report_data\" \n",
    "dbutils.widgets.text(\"myinput\",\"microsoft;apple\")\n",
    "dbutils.widgets.text(\"startdate\",\"2022-05-01\")\n",
    "dbutils.widgets.text(\"enddate\",\"2022-05-02\")\n",
    "\n",
    "var_a = dbutils.widgets.get(\"myinput\")\n",
    "var_a = var_a.split(';')\n",
    "print(var_a)\n",
    "start_date = dbutils.widgets.get(\"startdate\")\n",
    "end_date = dbutils.widgets.get(\"enddate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61e8c5fc-2dfe-4918-8917-34243d699e40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[&#39;microsoft&#39;]\n",
       "\n",
       "Loading and cleaning all data\n",
       "20220101 0000\n",
       "I am here\n",
       "(1085, 27)\n",
       "(1085, 14)\n",
       "done c=0\n",
       "20220101 0015\n",
       "I am here\n",
       "(874, 27)\n",
       "(874, 14)\n",
       "appending\n",
       "20220101 0030\n",
       "I am here\n",
       "(964, 27)\n",
       "(964, 14)\n",
       "appending\n",
       "20220101 0045\n",
       "I am here\n",
       "(1014, 27)\n",
       "(1014, 14)\n",
       "appending\n",
       "20220101 0100\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[&#39;microsoft&#39;]\n\nLoading and cleaning all data\n20220101 0000\nI am here\n(1085, 27)\n(1085, 14)\ndone c=0\n20220101 0015\nI am here\n(874, 27)\n(874, 14)\nappending\n20220101 0030\nI am here\n(964, 27)\n(964, 14)\nappending\n20220101 0045\nI am here\n(1014, 27)\n(1014, 14)\nappending\n20220101 0100\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "output = download_company_data(base_dir, start_date, end_date, var_a)\n",
    "dbutils.notebook.exit(output)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": -1,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "esg_final_one1 (3)",
   "notebookOrigID": 1451327037292386,
   "widgets": {
    "myinput": {
     "currentValue": "microsoft",
     "nuid": "d665e895-3192-4cd3-bdca-caf8ac4bc19b",
     "widgetInfo": {
      "defaultValue": "microsoft",
      "label": null,
      "name": "myinput",
      "options": {
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
